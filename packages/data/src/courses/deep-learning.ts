export default {
    slug: 'deep-learning',
    id: 'external-import-CS-7643',
    _createdAt: '2022-07-05T23:23:15Z',
    _updatedAt: '2022-08-25T21:38:34Z',
    isFoundational: true,
    _type: 'course',
    name: 'Deep Learning',
    officialURL: 'https://omscs.gatech.edu/cs-7643-deep-learning',
    tags: ['DL'],
    _rev: 'vLev53Cwnp4JOI2CCP4dKQ',
    codes: ['CS-7643'],
    description:
        'This course will cover theory and practice of deep learning, including neural network and structured models, optimization algorithms, and applications to perception and Artificial Intelligence.',
    _id: 'external-import-CS-7643',
    isDeprecated: false,
    syllabus: {
        file: {
            _type: 'file',
            asset: {
                _type: 'reference',
                _ref: 'file-db6e60912f386c5503f491772e5cb68dbc8df70c-pdf',
            },
        },
    },
    creditHours: 3,
    textbooks: [
        {
            name: 'Deep Learning',
            _key: 'fb80f17c253653b5fab765e873f8376b',
            url: 'https://amzn.to/3pO6pAw',
        },
    ],
    programs: [
        {
            _type: 'reference',
            _key: 'bcf529537e0c',
            _ref: 'b6f2bf84-c2ea-405e-8423-c348e1a94051',
        },
        {
            _ref: '054194e9-2449-4b4e-a000-532509e967c4',
            _type: 'reference',
            _key: '54950a2f1bb9',
        },
    ],
    reviewCount: 98,
    rating: 3.989795918367347,
    difficulty: 4,
    workload: 18.96938775510204,
    reviews: [
        {
            user: 'mhCfn5wF3DOFzGlaGSYD+A==',
            reviewDate: 'August 15, 2024',
            semester: 'summer 2024',
            review: 'Very interesting topics and sets up a good foundation for those pursuing ML specialization. However the course is very demanding and requires at least 20 hours per week. The assignments and quizzes are quite daunting. The assignments take weeks to complete and are not easy at all, leaving very little time to prep for the quizzes. The project and the theory parts of the assignments was graded leniently though.',
            rating: 4,
            difficulty: 5,
            workload: 21,
        },
        {
            user: 'Bi1d9Zzy/nPldLgkW5hvug==',
            reviewDate: 'August 12, 2024',
            semester: 'summer 2024',
            review: "Previous courses: KBAI, Simulation\nGrade: A\nThis is my third course in OMSCS program and I enjoyed it a lot. I've been working in data science for 2 years so the math part for this class wasn't too hard for me. The first assignment did scare me a little because we had to code neural networks from scratch using numpy, which is something I've never done before. The assignments do get easier though because we are allowed to use pytorch for the rest of the assignments. The professor did a great job explaining the concepts and the implementations. The TA hours are extremely helpful too, especially if you feel stuck on certain part of the assignment. Overall, this is my favorite course among the three I've taken.\nOne thing I want to complain is the course video quality taught by the Meta team. It was really really bad. It was hard to follow, especially in the lesson for word embedding. They just read the slide scripts and they can not even read them fluently. I understand that the Meta team is specialized in conducting research, not teaching. But I would rather this part of the course to be taught by someone who's less knowledge in the field but better at teaching people the concepts and using examples to help students learn better.",
            rating: 5,
            difficulty: 4,
            workload: 12,
        },
        {
            user: '6v6NWG6Kl/hPv2eJJuS8gA==',
            reviewDate: 'May 9, 2024',
            semester: 'spring 2024',
            review: 'The best part of the class is the subject matter, which is super cutting edge. The assignments are for the most part really good, though the NLP one felt less rigorous(but I still learned a lot!). The material is challenging and mastering it is a huge confidence booster.\nOverall the way the course is run is pretty low hassle, you get your grades back quickly, and there’s not a ton of ambiguity over what you need to do to get good grades on the homework.\nThe lectures are dreadful, the worst of any class I’ve taken here at OMS by far. Watch Justin Johnson’s lectures from his University of Michigan Deep Learning instead, and only come back to the class lectures right before the quizzes to see if you can recover anything you can use. Some of the papers that they assign as reading are very interesting though.\nThe quizzes are as bad as the lectures, super random feeling and it’s difficult to prepare for them. Thankfully they don’t make up too much of your grade.\nMake sure you’re coming into this class with a decent knowledge of multivariable calculus (derivatives, not integrals) and probability. Also, definitely also take ML before taking this class.\nFor me, DL was a hard class, but overall considerably easier and less time commitment than ML was, though they are difficult in different ways.\nPicking a rating for DL is tricky. In many ways it’s been the most exciting class that I’ve taken at OMS, but it’s also got some conspicuous flaws, so I can’t exactly rate it as highly as something like ML or RAIT that were just solid all the way through. In the end DL is really saved by the awesomeness of the subject matter.\nPrevious classes - ML4T, RAIT, ML\nFinal Grade - A',
            rating: 4,
            difficulty: 4,
            workload: 19,
        },
        {
            user: 'tWoDXZoAjQ9qXJlFiIBG/Q==',
            reviewDate: 'April 4, 2024',
            semester: 'fall 2023',
            review: "This course's design is like somewhat survey class and the instructor wants to cram all things at once in a single video.\nYou have to look for other alternatives online to better understanding what the lecture wants to teach you.",
            rating: 2,
            difficulty: 5,
            workload: 15,
        },
        {
            user: 'z9NQv9C9iU8cniecUaWM/Q==',
            reviewDate: 'January 31, 2024',
            semester: 'fall 2023',
            review: "Also posted on OMSHub,org\nI took the class in Fall of 2023 as my 6th class in OMSCS.\nOverall I really enjoyed the class and got an A but just barely.\nThe class consisted of 4 assignments and 1 group project project, and 5 \"quizzes\".\nPrerequisites:\nPython proficiency, especially comfort with numpy python package since pytorch uses a similar syntax.\nFamiliarity with machine learning. If you don't have this, I highly recommend taking the time to do Andrew Ng's machine learning or deep learning specialization on Coursera.\nAssignments I had to work on the assignments almost every day. They were very hard but if you were consistently working on it, checking EDstem, and office hours you could definitely get through them and learn a lot.\nAssignment 1 + 2: Deep learning basics and Convolutional Neural Networks from Scratch. I think the most useful thing I learned was how to do back propogation by hand and getting comfortable with using Tensors in pytorch.\nAssignment 3: Shortest assignment. Style transfer and visual explanation of deeep neural networks.,\nAssignment 4: NLP basics, RNN, LSTM, and Transformer Architecture. This IMO was the most interesting assignment. Language models like ChatGPT is built on transformer architecture so understanding them is very important.\nQuizes: IMO these were more like exams and the most stressful part of the class. You absolutely need to study for them. I did about average on these but I feel like quizes don't always reflect the assignments or the lectures very well.\nProjects: Your group has to do a deep learning project. My team did a kaggle competition where we looked at an image classification task for very large images (file sizes of >1GB). Kaggle is great because they provide free GPU resources (up to a certain amount per week). The class also gives you some GPU credits on Google Cloud but it is a very limited amount. We also had to submit a 6-page paper written in Latex document which is useful for those interested in publishing their results. The grading on this was very minimal. We had to submit the assignment within a few days of the end of the class so the TAs did not grade the report that harshly.",
            rating: 4,
            difficulty: 5,
            workload: 20,
        },
        {
            user: 'mPajKgOPQ6pV3UfAxJIgog==',
            reviewDate: 'January 5, 2024',
            semester: 'summer 2023',
            review: 'A well delivered class. Doctor Zsolt is a rigorous and curious person who inspires his students to be the same. Highly recommend.',
            rating: 5,
            difficulty: 3,
            workload: 10,
        },
        {
            user: '7RqJJOi7Q/l7bB59mDCwgg==',
            reviewDate: 'December 18, 2023',
            semester: 'fall 2023',
            review: "Deep learning was a good, but also mixed experience. As a disclaimer I've finished the course and have an A. I'd like to keep this review constructive for future students, so I'll focus primarily on how I'd approach the class knowing what I now know and general sentiments. Prior to this I had taken ML.\nAssignments were hard and the most time-consuming part of the class, but I would consider them a 4/5 in quality and where 70% of the learning happens. I think assignments could be improved significantly by breaking up deliverable pieces into even smaller chunks. Some of the smaller pieces we needed to put together didn’t always have a way to check that we did it correctly, making it dubious if the larger “whole” would work correctly. I found A1, A2, and A4 the hardest, with A2's CNN Backprop being particularly challenging. I do appreciate that code feedback via Gradescope was immediate, which compared to ML was refreshing. Knowing what I now know, I'd use the existing unit tests to get a solid conceptual understanding of what is being asked of you. Once you've got that down, the coding itself to match your mental model usually isn't too bad. I learned PyTorch during this course, and the learning curve was quite approachable. Honestly PyTorch feels easy after implementing forward passes, convolution, and backprop from scratch a few times.\nQuizzes were for me the least educational part of the course. Since the course isn't curved, they can also be the most stressful part if you're aiming for an A. Calling them \"quizzes\" feels a bit disingenuous, given that a single \"quiz\" might cover around 1-2 modules worth of lectures (in other words, 2 quizzes is roughly a midterm exam in my opinion, and there are 5 total in the course). The first three quizzes felt relatively fair, and the TAs gave helpful bulleted lists of topics that would be covered. The TAs did the same for the last two quizzes, but they still felt really pedantic in what they asked you to remember from (in my opinion) low quality Facebook/Meta lectures. Lecture videos were typically around 10-20 minutes, and you may have around 15-20 of them in a given module. The quiz might have a question on one specific slide that is covered for a minute or less, so it can feel like keeping track of all the needles in a hay stack. There's a review further down that says the quizzes serve no purpose other than to create an \"arbitrary grading curve\" or something like that, and that feels true to my experience. Having taken the course, the only real way to prep for the quiz felt like re-watching the lectures right before taking it. Math (mostly Calculus) was heavy in the first month of the course but much lighter thereafter.\nWhich I think transitions nicely into lectures. Lectures at the start of the course are pretty good, but then the quality gets progressively worse especially with Facebook/Meta. After the first half of the course, I began watching UMichigan's lectures on YouTube (by Justin Johnson, former TA for Stanford's cs231n which this course is modeled after), and that was the best approach for learning the material for the assignments. Then, say ahead of a quiz, you should watch this course's lectures just to make sure you get all the details you might be quizzed on. While I think talking over lecture slides is okay for some content, it gets pretty exhausting and hard to pay attention to after a while, especially when videos can be around 20 minutes. The UMich lectures have very helpful animations and student questions incorporated throughout, which makes for a more engaging learning experience.\nAs for TAs, I really appreciated Sangeet and Farrukh, whom I thought did an excellent job with their office hours and answering questions on Ed! I generally thought the TAs were pretty good for this class. My only gripe is they sort of actively discouraged making Ed a useful forum for conversation, since any tips to classmates could be considered offering too much advice. Hence Ed itself was pretty vague and not as helpful as it has been for classmate discussion compared to past classes I've taken.\nI wasn't a big fan of the group project. I think if Quiz 4/5 were merged (and the staff got rid of the pedantic questions within those), and then made the group project into an individual project, the course would be so much better. The project itself was critical to my learning -- until then I hadn't ever done the end-to-end deep learning process all the way from data collection to model. The assignments (for better or worse) skew heavily towards forward/backward pass logic, so this learning really uniquely came from the project for me. But I could definitely do without the \"group\" part of it, which I felt added a ton of unnecessary overhead. It's worth noting that if you hit all the points in the rubric, they grade it pretty leniently.\nFinal thoughts are: This class covers some of the most important material in Machine Learning. I personally think it should be required in the ML spec, and it has lots of potential to be the flagship course for OMSCS ML. I put in a ton of effort into the class, and (with the exception of the quizzes), felt that I mostly got out what I put in.",
            rating: 3,
            difficulty: 5,
            workload: 20,
        },
        {
            user: 'dA4eQAItCM26TKShs2W0hg==',
            reviewDate: 'November 17, 2023',
            semester: 'fall 2023',
            review: "I've really enjoyed this course. Personally, this subject matter is the whole reason that I began the OMSCS degree in the first place. The class gives a great place to learn the basics and the assignments really bolster the knowledge gained in the class. The final project is a great excuse to learn about a cool topic of your choice and develop something using the knowledge you've learned in the course. It is also quite open ended so you can end up sinking a lot of time into it. I also found that the readings in the assignments to be very enjoyable as they forced us to engage with research papers deeply.\nCritiques of the course:\nAs everyone else has said, the quizzes are brutal. They don't feel particularly fair and get increasingly difficult throughout the course as they cover more and more information.\nThe instructor lectures are fine, at some times confusing, but ultimately understandable. The Meta lectures really lack depth and I feel leave much to be desired.\nPersonally, I found this course to be very enjoyable, largely because I do not care about my grade and have had a great time working on the assignments and project. However, the quizzes really leave a bitter taste in my mouth, and for that reason I am rating the course a 4. I think if you didn't come into this course as excited about Deep Learning as I did, the quizzes and lectures could leave you seriously jaded, however I feel that the assignments are the exact opposite of the quizzes, straightforward, with clear expectations and satisfying learning outcomes.",
            rating: 4,
            difficulty: 5,
            workload: 18,
        },
        {
            user: '9QOIjL5VKTUIEccZE+02sA==',
            reviewDate: 'August 16, 2023',
            semester: 'summer 2023',
            review: "Amazing!!\nI had the time of my life doing this course. The subject matter is pretty relevant and recent. You get to do a lot of reading, and a lot of scratch implementations of basic compute units like RNN, MLP, and CNNs. I'll go ahead and say that you don't need to do a CV course if you've done this course well, but I'll not say the same thing about NLP, deep RL. Coming on to the logistics, there were three assignments and a final project. The assignment and project make up about 80% of the course, each weighted 20%. Grading is pretty lenient, if you pass the auto-grader on grade scope, you are guaranteed full marks in the coding section of the assignment. For writing, the grading is not as aggressive as ML. As long as you explain the results succinctly within the realm of DL, you'll hardly lose any marks. Quizzes are a little tricky and rely on you mugging up the tiniest of details given in lectures. Overall, I would like to stress that it is one of the most updated courses I've ever done. We were reading research papers not older than the past 5 years and papers which were very relevant to how current research is progressing. The only part I didn't like was the Deep RL. I feel it was kind of rushed and required a separate class to be understood.",
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'szFGSy34W3OiXNupCVHubw==',
            reviewDate: 'August 15, 2023',
            semester: 'summer 2023',
            review: 'The course material is boring.\nCourse material is not close relvant with homework.\nNeed a lot extra effort to finishe the homework',
            rating: 2,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'nxSQb6FOcVHHJT2ueblSNQ==',
            reviewDate: 'August 14, 2023',
            semester: 'summer 2023',
            review: 'It could have been much better, but, still, this is a good course.\nPROS:\n\nthe content is very deep,\nthe content is very very advanced,\nthe content is very up-to-date,\nYou will learn a lot,\nProfessor and TAs are charming and helpful,\nyour laptop/computer might not be powerful enough to run your final project\'s model, but, still, can still run it with fewer parameters/epochs, explain it well, and get the full mark for the project.\nThey give some Google Cloud Platform credits, that you might be able to use to run some GPU-based algorithms. If you are able to make these work, of course (not trivial). Google Colab is also an option (remember to select GPU or TPU).\n\nCONS:\n\nas they already warn you, please, AVOID IT if you do not have enough ML knowledge. I actually would recommend you to take, at least, the Andrew Ng/Deeplearning.ai DL courses/specialization before.\nyou will have to do derivatives for some exercises, and present in many papers.\nThe videos are way too long, some become very difficult to follow, gathering too many subtopics that could have split into shorter videos.\nBased on PyTorch, the Facebook framework.\nMost Facebook Researcher videos are very helpful, but some others are very difficult to follow: just "reading", and not even properly, with pronunciation issues...\nAdditional: you might do all the code, pass all the Gradescope tests, and then you discover that the report is based on stuff you need to implement apart from that. Not the course\'s fault, but just avoid being a procrastinator and sloppy as I was, or you will not get an A.\n\nSummary: I recommend it, but think of it as a very advanced course, you need to know a lot before, and it will be hard.',
            rating: 4,
            difficulty: 4,
            workload: 22,
        },
        {
            user: '0xXILfNPVGiaxf3oyRagHQ==',
            reviewDate: 'August 8, 2023',
            semester: 'summer 2023',
            review: "6th course. I really wanted to like DL but it ended up being my worst experience in OMSCS so far.\n\nQuizzes are a complete shitshow. It feels like these were originally created to be open book, then made closed book to artificially make the course harder. The amount of material covered by one quiz is insane and the questions aren't surface level, but small details you could easily miss if you don't have photographic memory.\nLectures are bad overall, especially the ones created by Meta. The pace is extremely fast (especially when every small detail could be on the quiz), lecturers often have strong accents that make them impossible to understand.\nGroup project: it's a hit or miss. I was lucky to be in a team with no slackers. The free-for-all process of team formation is time consuming and annoying.\nAssignments are very time consuming (start early!). The programming parts are interesting and rewarding. Theory questions not so much, again it feels like they used these to make the course harder.\nOverall, I wouldn't take this course again. The material is very interesting, but the course itself needs some serious revamping.\n",
            rating: 2,
            difficulty: 5,
            workload: 20,
        },
        {
            user: 'Y0p+1lfk2jRxT+Y8MpA7lA==',
            reviewDate: 'August 7, 2023',
            semester: 'summer 2023',
            review: "This was my 6th course in the program and I found this as the most difficult class so far. I entered the class with a decent understanding of machine learning and basic understanding of neural networks but I struggled in the course right from assignment 1. The biggest challenge that I faced was to wrap my head around the dimensions of the input and output batches. Proficiency in linear algebra (particularly Matrix operations) is very important for this course. I ended up getting a B in this course (final score including the project was 88.6 and the cutoff for A was 89) but I have learnt way more than I hoped for. This course makes you work hard but teaches you a lot.\nPositives about the class include, excellent assignments that reinforce the learning, amazing instructor (professor Zolt) who is highly involved, very updated course content and thought provoking industry lectures.\nFor me the two biggest negatives were the quizzes and the lectures in the second half of the course. We had 5 quizzes in the summer term and to succeed in the last 3 quizzes required non trivial amount of mugging up stuff from the slides. The quizzes ended up costing me the A grade as I lost 6.3 marks out of the total 18.75 marks allocated to the quizzes. While the lectures in the first half of the course were excellent the ones in the latter half were not very good. Maybe it's me but I don't think talking over slides is the best way to teach cutting edge and highly technical subject matter. I think I will have to revisit that portion of the course again in the future.\nTo sum up,  this is a very rewarding and highly recommended course regardless of what specialization one is pursuing. However, the course can be made even more enriching if some of the issues are addressed.",
            rating: 4,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'vn6EeoAIJ3qVb/G909qosQ==',
            reviewDate: 'June 15, 2023',
            semester: 'fall 2022',
            review: 'I had reviewed this course earlier but now I have to come back to add preparation for this course:\nRead the book "Deep Learning from Scratch" by Seth Weidman; you can login to Oreilly and start reading. This is very well written text book that will take you through the fundamentals that gets taught in this course. Even after taking this course, I found this book to be filled with gold; So far I am close to getting done with Chapter 2 but it is just WOW!\nIf you find chapter 1 to be confusing, it might help to listen some DL lectures first and come back to this book. You really want to grasp the material of Chapter 1 and 2 for this book.',
            rating: 2,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'j2JE9EYpNSRq8bddc3IB+w==',
            reviewDate: 'May 11, 2023',
            semester: 'fall 2022',
            review: "This course has good and bad parts. Let me begin with the bad part. The quizzes, especially the computational parts, were difficult for me. I had to attend office hours to get help, especially for things that were likely to be on the quiz. It would be helpful to have more examples and explanations so that I could understand the math better. The quizzes sometimes tested theoretical material from lectures, which is okay, but I didn't always enjoy it.\nI will still give a rating of 4 out of 5 because it's an excellent course. Professor Zsolt and the head TA were fantastic, and they covered many relevant papers. The first half and last part of the course were amazing, but the style transfer section wasn't as good. The assignments, quizzes, and final project determined the grade, and they were based on the different modules covered in the course. The TAs were enthusiastic and made sure we not only understood the material but could apply it in our careers.\nModule 1: Introduction, Linear Classifier and Gradient Descent, Neural Networks, Optimization of Deep Neural Network, Data Wrangling\nAssignment 1: Building NN from scratch, fantastic assignment to learn basics.\nModule 2: Convolution Neural Network, Convolution and Pooling Layers, CNN Architectures, Visualization, Scalable Training, Advanced Computer Vision Architecture, Responsible AI\nAssignment 2: Building CNN from scratch, building CNN with Pytorch and then comparing results.\nAssignment 3: Visualizing CNN, Style Transfer\nModule 3: Structured Neural Network, Language Model, Embeddings, Neural Attention Model, Neural Machine Translation, Automated Speech Recognition\nModule 4: Deep Reinforcement Learning, Unsupervised and Semi-Supervised Learning, Generative Models\nProject:  My team focused on contract review automation utilizing Natural Language Inference (NLI) with an attention mechanism. Automation will not only shorten the review process, but it will also increase accuracy by removing factors of human mistake. This body of work may be utilized for various activities like as paraphrasing, summarizing, retrieving information, answering questions, and so on.\nI pursued computational data track from omsa more reviews here: https://www.linkedin.com/pulse/georgia-tech-omsa-program-review-sid-gudiduri/",
            rating: 4,
            difficulty: 3,
            workload: 15,
        },
        {
            user: 'u6Ht6glG/lCds9XTvylglQ==',
            reviewDate: 'May 8, 2023',
            semester: 'spring 2023',
            review: "This was a great course in terms of rigor and learning. Grading in general is lenient and queries are answered promptly. The best part about the course was TA's Office hour tutorials, while the worst part was quiz.\nTo excel in this course you should have a basic understanding of deep learning concepts. You can learn pytorch on the go.",
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'yDLjYZSnL0WZX+N59lDC+A==',
            reviewDate: 'May 6, 2023',
            semester: 'spring 2023',
            review: 'This is a tough course but also a must do course if your specialization is machine learning. Not sure why its not mandatory.  Do not pair it with another course ( I made this mistake and I might score a B in both) as it requires significant workload. The course moves really fast between A1 to end of A3 and right upto A4. I did not enjoy the group project but I felt other assignments such as style transfer and transformer architecture in NLP were upto latest standards. Prof Kira makes a lot of effort to make sure students get to learn the latest tech. The quizzes are super hard and so are assignments, except assignments are open book. I loved the paper reading section of the assignments in addition to coding and justifying results. I did this course right after Machine learning and boy, this was definitely a notch up. The writeups were a bit informal and rubric was not hidden unlike ML, however, it was still a lot of work but deeply gratifying in the end.  This course is a copy of CS231N from stanford course material wise. I would rely on videos from CS231N as videos from Meta are total crap.',
            rating: 5,
            difficulty: 5,
            workload: 28,
        },
        {
            user: '3DAMyikTEhmL6xCnN9OIKw==',
            reviewDate: 'May 4, 2023',
            semester: 'spring 2023',
            review: "Final grades have not come out yet for Spring 2023, but I wanted to write my review based on the experience of the course and not based on my final grade coming out of the class.\nAs someone with a non-CS but rigorous STEM undergrad background, I wanted to take DL in order to gauge whether I had the chops to do semi-theoretical work in the field of artificial intelligence and deep learning. This course was very eye-opening in that it forced me to understand the math behind the theory and to write from scratch the application of theory for neural networks, stochastic gradient descent, etc. It's made my mind much stronger as a result, and I have a better appreciation now for the large scale models that are dominating all the news articles in the world (looking straight at you, GPT-3 and GPT-4).\nHowever...I really abhorred the group project. I had to work with well-meaning individuals who lacked self-direction. They looked to me for every single direction, and what had been quite an enjoyable course took a rather sour turn in the last 4 weeks because of the group project aspect. Working in a group with people whose work habits are unknown to you from the beginning is a recipe for frustration and discontent. I would recommend that a 5th individual coding+writeup assignment take the place of the group project for a better learning experience on an individual basis. In fact, I would rather have 7 quizzes in the semester (instead of the 5 quizzes) than to have to do a group project. I disliked the quizzes because they did not foster my learning progression as much as the individual assignments, but in comparison to the group project the quizzes were wonderful. That's how much I abhorred the group project.",
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'vn6EeoAIJ3qVb/G909qosQ==',
            reviewDate: 'April 18, 2023',
            semester: 'spring 2023',
            review: "This course and the material presented is interesting one but the level at which it has been presented leaves student with need to scour the internet for additional materials. With that said, it is absolutely possible to do well in this course because of the power of internet.\nGuide to this course:\n\n\nCalculus! Calculus! Calculus!\nYou really need to know how to multivariable calculus to survive this course; Khan Academy course will help you if you need help. Look for derivatives, chain rule and multi-variable differentiation (partial derivative). However, nothing complicated here beyond just knowing derivative!\n\n\nDo you know what is Hessian matrix?\nOnly the name is intimidating but not super complicated once you know #1 from above. Read this https://explained.ai/matrix-calculus/, at least all of Matrix Calculus. Even if not all things are clear for now, move on to bullet point #3.\n\n\nGetting started before semeseter:\nI strongly feel that you should watch the lecture by Dhruv Batra - another professor here at GATech at least up to lecture 8 before semester starts. That way you get the taste of the material and how much calculus to know. Here is the playlist: https://www.youtube.com/watch?v=x2lRaIeCRV8&list=PL-fZD610i7yB7gDnPDpFcKpHI9X8z3OQ7\nand schedule: https://www.cc.gatech.edu/classes/AY2021/cs7643_fall/\n\n\nNow that #3 is done, try numpy tutorial:\nhttps://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html\n\n\nPytorch tutorial:\nwhile you could learn on fly, I feel that knowing pytorch and trying things will help you. I strongly recommend following this course which also covers pytorch:\nhttps://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51\nBy the way there is also book for this course by same lecturer available on https://learning.oreilly.com/library/view/machine-learning-with/9781801819312/\n\n\nThe course lecture offered here feels like a high level overview useful for review right before quizzes but not sufficient on it's own. Here is list of additional materials that I have used over the period of course (not in any particular order):\nUniversity of MI DL:\nThis is enhanced lecture from cs231n\nhttps://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/schedule.html\nhttps://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r\nGA TEch Dhruv Batra:\n\thttps://www.cc.gatech.edu/classes/AY2021/cs7643_fall/\n\thttps://www.youtube.com/watch?v=x2lRaIeCRV8&list=PL-fZD610i7yB7gDnPDpFcKpHI9X8z3OQ7\n\nSabastian Raschka:\n\t- this course has nice lecture + code implementation in PyTorch\n\thttps://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51\n\nThis series was useful especially for lecture module 13:\n\thttps://www.youtube.com/playlist?list=PLaZQkZp6WhWxIvz74aEvvVc99o7WuOoQ6\n\nRL Lectures useful playlist:\n\t- https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n\nNPTEL Lectures:\n\t- lots in details:\n\t- ML + DL + math all covered here\n\thttps://www.youtube.com/playlist?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K\n\nNN Backpropagation and GD explained:\n\thttps://youtu.be/CoPl2xn2nmk?list=PLktLqLNJ8YJbIv1TCOY-9d8-yQGUpYbMa\n\thttps://youtu.be/8d6jf7s6_Qs?list=PLktLqLNJ8YJbIv1TCOY-9d8-yQGUpYbMa\n\nPathsala playlist:\n\thttps://www.youtube.com/playlist?list=PLkhxeo3AyR-xrggjA0DgWpbWZMTwBy3dV\n\nJeremy Howard course:\n\thttps://course.fast.ai/\n\nAndrew Ng:  \n\tlooks like some content difference but has practical tips\n\n\thttps://www.youtube.com/playlist?list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc\n\thttps://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0\n\n\nlectures + notes + tutorials on PyTorch:\n\thttps://uvadlc.github.io/\n\nCMU LECTURE:\n\thttps://www.youtube.com/playlist?list=PLp-0K3kfddPwJBJ4Q8We-0yNQEG0fZrSa\n",
            rating: 5,
            difficulty: 4,
            workload: 25,
        },
        {
            user: '5OF4BxzgwHSPRo/ztUu4pA==',
            reviewDate: 'March 26, 2023',
            semester: 'summer 2022',
            review: "Lectures are ok, goes through a lot of both high level and technical concepts. Sometimes felt like quiz material wasn't super well in the lectures though, I think this would be improved with more example problems being worked out in lectures.\nAssignments were moderately challenging and were very dependent on understanding lecture material. Office hours are a must to get some extra nuggets of info for assignments if you don't immediately understand where to start (they were recorded).\nGroup project was tough to get started on with picking a topic, and it's super freeform in requirements. In my opinion our implementation sucked performance wise, but it was pretty easy getting a good grade by explaining the \"why's\" to our results.",
            rating: 3,
            difficulty: 3,
            workload: 10,
        },
        {
            user: 'QAay9WOE4LB0nDCeidwhhQ==',
            reviewDate: 'January 12, 2023',
            semester: 'fall 2022',
            review: 'My Background: 3+ yrs Python programming, 2 yrs+ work experience in DS, basic knowledge of calculus\nLectures: The computer vision part of lectures is clear and well-designed, and related quizzes content is mostly covered by the lectures and tutorials. However, there are some contents not taught by Professor that are hard to grasp (e.g. state-of-art NLP topics), those lectures kinda lack details and are more like general talking\nQuizzes: It tests your deep understanding of concepts and some calculation questions. The tutorial class is very helpful.\nAssignments: They build the neural network from scratch, you need to be good at numpy and pytorch, the dimensionality for calculation is hard to get at the beginning.\nGroup Project: Start as early as possible, need time to learn GCP platform for model training with GPU, and need time to do experiments for various model approaches\nOverall, this is a good course and covers important topics in deep learning, you will learn a lot and start to explore some real-world DL problems.',
            rating: 4,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'yfCyfJzoGzko0Pt8t2HI1g==',
            reviewDate: 'January 7, 2023',
            semester: 'fall 2022',
            review: "I took it for last Fall semester. Overall, I learned a lot and loved this course.\nFirstly, the workload was super heavy. Every weekend, it's either quiz due, or assignment due. With the fast deep learning field growing, the course covers more and more contents. ;)\nThe good thing is I can tell professor really want us to gain essential knowledges, especially the newest DL techniques, as the paper for reviewing in the assignment has been updated into 2022.\nFinally, alert about each assignment, there are paper reviews, answer mathmatical questions, different NN code from scratch, parameter tunning and model training, then report. Please perpare at least 40 hours for each assignment, and leave enough time for model training. DON'T WAIT UNTIL THE LAST WEEKEND.",
            rating: 5,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'pgvpTKaeXNbLqtR72bHIIQ==',
            reviewDate: 'December 24, 2022',
            semester: 'fall 2022',
            review: "I don't have CS background. This is my 2nd course in the program and the first one was ML4T. As its syllabus says, without previous knowledge on ML, it is hard to follow the course. (I took ML course in other institution) I don't think that ML4T alone will prepare you to this course.\nLectures usually cover a lot of things in some pages of slides, so you may need to search topics on the web to fully understand the concepts. So, lectures and class slides are usually not enough to understand the materials.\nQuizzes are brutal but it takes only small portion of your total grade. The instruction team provides a lot of reading lists, usually papers, and you need to know the details from those lists to ace the quizzes, which is not necessary.\nAssignments takes the most parts of your grades and you need to put quality of time. As other said, start earlier and aim toward to get full credit from gradescope auto grader. In this way, you will be safe for A even though you loose some points from writing parts of your assignments.\nGroup project is also take 20%, so it's a lot. I luckily got great members who all decently participated in the work. However, we started the project after the last assignment, so we need to rush and put a lot of time getting closer to the due. The final project grading is lenient considering the distribution of scores.\nOverall, I learned a lot, am satisfied with the course and ended with A.",
            rating: 5,
            difficulty: 4,
            workload: 16,
        },
        {
            user: 'YSmR2Wa53yPIhwqoL+t8Xw==',
            reviewDate: 'December 21, 2022',
            semester: 'fall 2022',
            review: "There is a lot of depth and breadth. You learn a lot. Assignments are good.\nVideos are problematic. A technical paper is explained in 1-2 minutes, and they move to the next one. I don't understand anything. Another problem is presentation is powerpoint style most of the time. Even then, it is possible to divide the same slide into a series of pops so that I would know where to look at. I think videos, particularly some of the meta videos, have a lot of room for improvement. I would suggest if you don't understand something in the video, you can look for an online summary of the paper on the internet. For any topic you don't understand, you have to dig deep on the internet. They will ask every detail on these topics in the quizzes so you need to make sure you understand.\nI think our final project was graded quite low and randomized. I do not blame the TA's. They had very little time. I would recommend a more regularized final project with better guidance so we know where we are at.\nI thank for the professor and TA's for their effort though. It was a great class and I got an A finally!",
            rating: 5,
            difficulty: 4,
            workload: 24,
        },
        {
            user: '76y1mAeCgF6iZTF2UiARtA==',
            reviewDate: 'August 14, 2022',
            semester: 'summer 2022',
            review: "Took this class after doing AI4R and ML4T and I loved it. Difficult and a lot of material to cover but my overall understanding of ML skyrocketed because of this class.\nThe lectures are difficult so I found it helpful to skim them in 2x speed and then rewatch them at normal speed while taking notes.\nThe projects are worth a significant amount so don't worry too much if your quiz grades are bad. Coming from ML4T where they would nitpick reports, the report grading system was refreshing. If you can pass the autograder you'll get an A on the report. No grading curve.",
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'ftRAEqpsI/BJsD8GedZ1vg==',
            reviewDate: 'August 14, 2022',
            semester: 'summer 2022',
            review: 'DL is my fifth course, and it is one of my favorite. Most of the lectures are very well-organized and insightful. It really gave me an idea why the algorithm is designed in that specific way.\nPaper review in the assignments is also a very good part. I got the chance to be exposed to very good research papers. The course is really good.\nHighly recommended.',
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'E3vZCJlepjZiwX/8XkVdyA==',
            reviewDate: 'August 7, 2022',
            semester: 'summer 2022',
            review: 'This is my second course in OMSCS. The Deep Learning course is very useful and insightful with great TAs. However, the course could be further improved by reducing the quizzes workload. Most quiz problems are not insightful and more conceptual test IMO. The assignments and final projects are very well organized and challenge you from all angles. I got an A in the course and really think you should take it in a full semester rather than summer.',
            rating: 5,
            difficulty: 4,
            workload: 8,
        },
        {
            user: '31cnB9ib7GA9rEmlpQdgvw==',
            reviewDate: 'August 7, 2022',
            semester: 'summer 2022',
            review: "This is my second course for OMSCS. The first course is ML4T. Deep learning is terrible. On prerequisite, it said it never be your first ML course. But it is the first Deep learning course for all the people.\nThe course never give details about the concept. Just formula and description. Just like the teacher tell you how to do the multiple. But the homework is calculus. I finally got a B.\nProbably, it should be better if you take machine learning then take this course. But I really struggle with this course. The quiz is difficult. If you don't understand the video. You will not perform well in the quiz.\nThe video doesn't that helpful. The most helpful course material would be the cs231n video.",
            rating: 1,
            difficulty: 5,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 8, 2022',
            semester: 'spring 2022',
            review: "My background: Handful of years experience programming in C. My previous classes were: Computer Vision, Intro to OS, Advanced OS, Intro to High Performance Computing, Human-Computer Interaction, Graduate Algorithms, and Machine Learning.\nThis is the single most practical course I’ve taken in terms of new skills I didn't have before, that I expect to use at work regularly especially for my side hustles. A close 2nd place is Intro to High Performance Computing.\nIf you want to learn the mathematical nuts bolts of ML models (implement from scratch with numpy) AND gain a working proficiency with Pytorch, this is the class for you.\nIt’s a slog with a lot of things to submit, but 70% of the class gets an A historically. The final group project had a median score of 58/60 (points, not percent). They graded 200-400 of them in 3 days and were not picky at all. Just touch on all parts of the rubric in some depth.\nMy average quiz score was 55%, but I got a 97% overall thanks to doing the extra credit assignment and spending a lot of time on the report/theoretical portions of the assignments.",
            rating: 5,
            difficulty: 3,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 7, 2022',
            semester: 'spring 2022',
            review: 'First of all, my comment is from a beginner’s view, please ignore if you have good amount of background in DL. About my self: This is my 7th course in OMSCS. I have taken ML4T but have no background in DL. I am not from CS or engineering major, but I work with big data and am comfortable with basic algebra or calculus. My final grade in DL is A.\nI definitely recommend anyone who works with data to learn DL, but this course is not the best way to start your journey. Although the topics sounds interesting and super useful, course quality is mediocre at best compare to others in OMSCS. Please lower your expectation and prepare for a lot self-learning. Overall, I would like to rate 5/5 for the topics, 4/5 for assignment 1 and 2, 2/5 for assignment 3 and 4, 2/5 for lectures and organization. The first half is challenging and fun, but then things start to fall apart drastically. I need to remind myself several times a week that the class is almost over, in order to not melt down in the last two months.\nPlease consider the following things before signing up. If you meet 2 or more of the descriptions below, this course will likely to be a rough ride for you.\n\nYou have never learned any DL courses or topics. (General ML courses only have limited help so they don’t count);\nYou do not know and do not like to learn linear algebra, multivariable functions, derivatives, etc;\nYou have never used pytorch or numpy;\nYou have other thing going on in life that constantly requires > 30h/week.\n\nOther reviews already mention most comments I would write. The biggest issue to me is this course tries to cover everything in one semester, but the lectures never spend enough time to explain concepts. DL is such an interesting topic, it is unbelievable how boring these lectures are. I ended up completely rely on Stanford CS231n which is a really great course, highly recommend. Another issue is they put a lot on the to-do list to fill up your time, so you might burnout and lose motivation quickly. I do appreciate there are a lot of office hours. Many thanks to TAs Alex Shum (assignment 1 & 2), Farrukh Rahman (assignment 1 & 2) and Sangeet Dandona (assignment 4). Do remember you need to attend OH to get hints and fulfill hidden requirements of assignments.',
            rating: 3,
            difficulty: 4,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 7, 2022',
            semester: 'spring 2022',
            review: "This is my 4th OMSCS course but I took the first 3 (including ML) back in 2015. In the month prior to the course starting I took Andrew NG's deep learning course which i felt was very good preparation for the course. In short, I felt I really learned a lot and my understanding is much deeper than what i could've gotten from the Andrew NG course alone. The quizzes involve a fair amount of preparation (taking detailed notes on lecture videos and spending a few hours or so studying the notes on quiz day). I think the questions were very fair on the first 3. The last 2 had a few nitpicky questions and while I didn't do as well on those, it didn't really impact my overall grade too negatively. Overall I felt that the quizzes in terms of prep time required were fair for a graduate level course. The projects were really great, but of course must start early and give yourself plenty of time. I didn't really bother doing any of the reading, and I agree the DL book is only useful if you already know the subject matter very well (so is therefore not useful for learning in a first course). My only real complaint, as stated by other students, is I really disliked the Facebook lectures. They don't explain things in decent lecture style detail and it feels as though the presenters are just cursorily describing algorithms and techniques. After watching all those, I watched the Andrew NG lectures and read some things on towarddatascience which gave me a an understanding that the lecture videos just did not. Professor Kira's lectures are pretty good, but I do feel it helps to watch Andrew NG's lectures to supplement and gain intuition.",
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'April 27, 2022',
            semester: 'spring 2022',
            review: 'This is my 7th class in the program and I took AI and ML right before DL. First half of the class (lectures + A1 & A2) were well organized and benefited my understanding a lot. Second half (starting with Meta lectures) and A3 & A4 were not that well organized. A very good aspect of this class is that the teaching staff have been the most enthusiastic that I have ever witnessed (they offered office hours more than all my other classes combined). Another highlight is that you get to have exposure to the most cutting-edge deep-learning technologies the industry and academia are using. The downside is the lack of coherence of the material that is being taught.',
            rating: 4,
            difficulty: 4,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'April 11, 2022',
            semester: 'spring 2022',
            review: "I have mixed feelings on this class. Overall, there's a ton of content and things to learn. Out of necessity, you will have to ramp up really quickly or it's easy to fall behind and play catchup the entire time.\nI won't reiterate everything but I agree with most of the sentiment from reviewers particularly those taking the course in Spring 2022: quizzes are unnecessarily hard with frequent curveballs, at times you feel like you're on an island for assignments which were pretty taxing at times, with some challenges not really benefitting your learning at all.\nAll that being said, there's current content in this class so it does a good job trying to keep up, as this field is constantly evolving.\nJust prepare to spend a LOT of time on this class to get the most out of it. Coding assignments were my favorite, lectures were not very engaging as others stated, especially the meta guest lectures. There were times where I felt like I'd never get through an assignment but so far (4/4), I have been able to get things finished (right up to the deadline).\nLooking at the current pain matrix I'd say an estimate of < 20 hours is pretty generous unless you already have DL experience of some kind.",
            rating: 3,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'April 11, 2022',
            semester: 'spring 2022',
            review: "Don't take this class unless you feel you must, its not worth it, either learn on your own or take a course elsewhere it will be a better experience.\nI want to preface this in that I have 5 years experience as data scientist focusing on deep learning, I have published research on deep learning prior to taking this class. I am on track to get an A.\nQuizes:\nI feel like these are designed to produce a grade distribution and serve no other function.\nThe TAs may feel like providing examples of what is on the quizzes, but studying will likely be largely pointless for anything beyond reviewing lectures and hoping that you can do the math they expect.\nClass experience:\nTAs can flat out delete posts that have any potential useful discussion just because they have extremely restrictive information sharing rules.\nThis stifles any useful discussion basically.\nProjects:\nA1, and A2 were wonderful experience of doing backprop from scratch, I really liked these. I really like the theory questions.\nA3 and A4 were exercises in coding to the autograder, I feel like this was a waste of time and money.\nThe test they provide for the homework's are brittle and incomplete, and some of the grade scope test are rather pointless be we are graded on them regardless.\nThere was an optional A5 that was rough around the edges but I really enjoyed the challenge here.\nGraded Discussion:\nThis are a waste of time, you read a paper, write answers to a few questions, and then respond to a few other students. Nothing here drives a genuine discussion, and despite how well you write your responses, the TAs find ways to doc you points without feedback on how to improve.\nFinal Project:\nThis works well in theory if you have good team mates, but if you have team mates that are unresponsive and don't do their share of the work its awful. Really this is why you should not take this class, it makes what could be an enjoyable solo project and makes it a crap experience. Seriously don't both with this class because of this.",
            rating: 1,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'March 22, 2022',
            semester: 'spring 2022',
            review: "Lectures are very dry and soporific. Quizzes are extremely difficult, testing random facts from lectures and readings. A lot of effort is needed to get high marks on them, and each individual quiz is worth little of the grade. Assignments are decent and are where the core learning happens, but the culture of the class discourages helping others and therefore hinders some of the learning. One TA straight up cancelled students' posts. The course overall just has poor organization, and the core learnings from the class can be done individually. Expert competency with numpy is assumed.",
            rating: 2,
            difficulty: 5,
            workload: 30,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'March 14, 2022',
            semester: 'spring 2022',
            review: "Pros:\nGreat assignment content, interesting to see the results of the implementations. They are very hard though.\nProject seems like a good opportunity to explore areas of interest in DL\nPytorch is nice to finally learn\nCons:\nVery dry lecturer. I'm sure the prof is a nice guy but he manages to make the lectures extremely boring on such an interesting subject. Very little passion in delivery, dry, monotonic.\nQuizzes. Why are these even a thing. It's an unbelievable stress inducer, they test extremely random factoids from the lectures and you pretty much have to know perfectly word for word what is presented on the slides. They are only worth around 4% each but add up.\nTA policing. As another review mentioned, the TAs are extremely annoying when it comes to policing the class forum and not only taking down things that could be considered \"too revealing of the assignment\", but also threatening to impose penalties! Why in the world do I have incentive to help on ed if I am at risk of getting a penalty just for simply suggesting to use one function over another? It's absolutely ridiculous.",
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'March 13, 2022',
            semester: 'spring 2022',
            review: "I really don't know what happened to this course this semester, but it's definitely been one to remember. We're halfway through this semester, but I wanted to go ahead and put a review up to warn others before they sign up for this course.\nFirst of all, this class has the worst group of TAs I've seen yet (and yes, that includes ML). They completely ignore questions, office hours are terrible, and the only thing they seem to be interested in is policing the project threads (if anything could even remotely be considered a hint, they remove it). The grading feedback is comical. You'll get something like \"we weren't expecting this,\" so you ask for additional feedback... they respond with \"this value wasn't what we were expecting.\" And be careful requesting a regrade, they will lower your grade in a heartbeat.\nThe class organization is freaking ridiculous. Assignment dates are constantly changing. You never know whether to believe canvas, grade scope, or the syllabus because they all three have different times. Then they randomly start changing due dates while the assignments are on going. They once changed the due date for a quiz to be 12 hours earlier so a lot of folks rushed to take it, then said it was a mistake and the folks that missed it got three additional days to take it. Also had a problem where they didn't make the calculator in one of the quizzes obvious, so some folks were able to get some math questions thrown out but not for everyone. The assignments page and grades page on canvas are still screwed up halfway through the semester and they don't seem to really care.\nSo 20% of the grade comes from five quizzes, and these things are BRUTAL. You can't just watch the lectures and feel confident enough to take the quizzes, you need to understand the material in every aspect, read the additional materials, and do some independent research, and hope to score at least a 50. Previous semesters only had math on 2 quizzes, but that's changed. There's at least 20-30% of the quiz that is heavy math and that's on all of the quizzes.\nThe projects are rip-offs from Stanford's version of this class. You know what that means? Right, the TAs don't have a clue what's going on with them. We've started project 4 and the libraries they are using don't even run on newer graphics cards (30 series). Their response? \"Just run it on Google colab and see if it works.\" Projects 1 and 2 weren't horrible. Project 3 required you to read 6 papers and attempt to decipher the algorithms (we had to beg for an extra week because they said this project was too easy and took a week away from us). Oh, and make sure you read the class forums, they'll hide hidden requirements about the projects in there.\nUnlike previous semesters, they are nit picking the write ups for the projects (without explanation of course). Make sure you do a PhD thesis for each of the questions. The grading is all over the place. You may be 0 points for your explanation but someone else got full credit with the same explanation. They've also increased the difficulty of the reports this semester and added math questions to them.\nGrading is SLOW. I mean months slow. They even completely forgot to grade an assignment until someone asked about it, then they pretty much gave everyone full credit if you turned something in (was only worth 0.5%).\nThe only improvement I have found this semester is they no longer test on the Facebook lectures. Thank goodness because those lectures are a joke.\nI'll complete this review when the semester ends, but if you're thinking about signing up for this class, you might want to think twice. This is my 8th class and I wish I wouldn't have taken it. It's been extremely frustrated dealing with this TA group and has made a hard class unbearable.",
            rating: 1,
            difficulty: 5,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'February 13, 2022',
            semester: 'spring 2022',
            review: "My background: As an OMSA student, I am taking Deep Learning as the last one in my program. I do not have a CS background and enrolled in OMSA for a career change to data science. Prior to this course, I took Andrew Ng's Deep Learning specialization on Coursera to get a high-level understanding of deep learning concepts.\nCourse itself: Although I am less than halfway through the course, this is already my favorite course in the OMSA program. Although I have attempted to study deep learning through MOOCs and hackathons before, this course gave me a deep dive into deep learning I needed to make all the concepts really stick.\nThe course is well-designed. TAs are very responsive and their office hours are good for getting unstuck. Their office hours include occasional tutorials dedicated to trickier parts of the assignment. The readings are interesting and easy enough for someone like me (no STEM background prior to OMSA) to follow.\nDifficulty: The most difficult parts of the course are the coding portions of the assignments but all assignments of the course have been doable. The theory portions of the assignments are easy. Although doing well on the quiz requires watching the lecture videos/doing the readings more closely, there have been no trick questions on the two quizzes I have taken so far.\nWorkload: Varies. I have been putting in 20-30 hours/week to finish coding portions of the assignments (there are four assignments total) well ahead of the deadline. On weeks where I am just watching the lectures or finishing up my assignment, the workload is around 5-10 hours.\nRating: I strongly recommend this course. I am glad I set my alarm early in the morning to register for the course back in December.",
            rating: 5,
            difficulty: 3,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'January 23, 2022',
            semester: 'fall 2021',
            review: 'I agree with the other students that this is the most "contemporary" course in the OMSCS journey. Most other courses only have the relevant research papers dated 5 to 10 years ago, but the amount of research papers on Deep Learning (DL) are really overwhelming in these few years (since the revival of neural network at approx 10 years ago). Meanwhile, this course is being actively updated on each semester, and it will guide you through even the newest/hottest topic like GPT-3.\nThe course is easier than I thought (though still hard) since the core algorithm is nothing but gradient descent. Of course there are many different networks and optimisers and schedulers, but it seems most of them do not involve complex math equations. As professor\nZsolt Kira said (and probably a sad fact), most neural network (NN) models are empirically found, rather than deduced from a math model.\nWhen your DL or NN models do not work well, it is this empirical nature of NN models that makes them hard to tuned or optimised. Often it is very hard (if not impossible) to mathematically prove a NN model is correct; the model still works somehow, but at sub-optimal quality. Life is even harder when DL needs massive computation power before a single empirical test can converge (i.e. a week on MacBook, days on Google Colab, hours on latest GPU), which makes the software development cycle extra long.\nRegarding GPU, the course organiser is very kind to invite Google and Amazon to offer few cloud computing credits to the students. However, Google Cloud Platform (GCP) only gives each student "USD50" while any public user can already receive "USD300"... on the other hand, Amazon came in very late when most students have already started the projects, so switching from Google Colab or GCP to Amazon Cloud might not worth the trouble. Also I am not satisfied about Google, since GCP charges me a much higher price (though subscription will only charge after USD300 is used up) with a 8-year old GPU Nvidia K80 offer after complex application, while Colab Pro can already give me a 5-year old P100 at a much cheaper price; though on the other hand Colab terminates my program execution every 24 hours even if I paid for the most expensive Colab Premium (with background execution). Not mentioning that Colab UI has issues (bugs?) that hide the "background execution" option from time to time, and allocates & shows the CPU / GPU memory incorrectly so that my program crashes on a memory-full error without any early warning...\nTests and challenges in the course are homework assignments, quizzes, and a project. The homeworks are fine, which progressively guide students through the DL concepts, and teach you how to use PyTorch (though it is best to run PyTorch in Colab to avoid rare local CPU errors). The project is not very difficult except on project management and choice of project objective. I was very lucky to have good teammates, one of whom was so heroic to consolidate all our writings into a big & elegant research paper. The project instructions mandate each students in the project team to work equal share on all tasks, though I still think it would be better for the project if each of us can assume a different role, e.g. programmer, writer, project manager, etc.\nNevertheless, beware of those "quizzes"! As many students said, quizzes in this course are never quizzes; they are full-blown exams!! And very difficult!! They are closed books and some concepts are very confusing, so I think most students lost most marks on the quizzes, rather than failing an assignments or project.\nRegarding the video lectures, I think most videos offered by professor Kira are great, but those offered by Facebook are usually (50% chance) poor... The problem is that some FB speakers speak strange English with difficult accent (which is hard to interpret when without subtitles), when they present diagrams or equations they do not explain or prove at all... Some materials are more like sales presentations than concept overview or worked examples. Sorry Facebook, I really cannot appreciate your video lectures (though I thank you for PyTorch).\nOverall, this is really a great course. Enjoy!!',
            rating: 5,
            difficulty: 4,
            workload: 30,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'January 15, 2022',
            semester: 'fall 2021',
            review: 'Excellent course as an intro to DL. Expect to spend a lot of time trying to figure out why your code does not work since TAs do not offer much help beyond "there is something wrong with your code."\nPros:\n\n\nProjects. This is where you truly learn the material. Yes, you will struggle with the code. Yes, the last assignment has incomplete unit tests and you will be re-writing the code for the sections you just finished and passed local tests. Having said that, this struggle is good in a sense that once you finish this class, you will have the confidence to teach yourself pretty much anything since there is no help offered.\n\n\nSangeet Dandona and Farrukh Rahman are two amazing TAs who actually know and understand the subject they are teaching. They explain the material clearly and, more importantly, they explain the intuition and not regurgitate how to code an equation. The rest of the TAs are not so great.\n\n\nProf. Kira\'s lectures are amazing. I wish he did all lectures and not outsourced to FB/Meta researchers.\n\n\nGraded discussions. At first I did not like it since it feels like you are forced to write a specific amount of words. Reading articles is crucial to keeping up with the developments in the field. At first you read because you have to, later - because you want to. Writing responses to comments of others forces you to think deeper about the subject you just read. Overall, I think this is a good pedagogical tool.\n\n\nCons:\n\n\nFB/meta lectures. These are simply pedagogical disasters. Do not expect to learn anything from them. Read comments of other reviewers about this subject, they are pretty accurate.\n\n\nQuizzes. I am not sure what is the point of them outside of being a grade differentiation. The quizzes cover a spectrum of topics and ask fairly detailed questions. People who claim to do great on quizzes are the ones who work in the DL domain as their regular job or have some exposure to this domain. It is not that the quizzes are hard, but given the amount of time you spend on projects, you will have less, if any, time to prepare for these if you have a career and a family.\n\n\nThe class is front-loaded. I put on average of 20 hours per week but the distribution is not even. There were weeks I spent up to 60 hours for some projects, then there are weeks you do not do anything at all towards the end of the semester.\n\n\nGroup projects. You will have a free-rider problem.\n\n',
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 28, 2021',
            semester: 'fall 2021',
            review: "Overview\nFor context, this was my seventh course, having taken AI4R, RL, AI, ML, BD4H, and ML4T beforehand, in that order.\nDeep Learning is a course which I have very mixed feelings on. The material is very interesting and well worth learning, but the course itself is highly variable in quality and feels like it has artificial difficulty.\nDeep Learning covers over the course of the semester the following material, All the material is quite fascinating and can feel like drinking from a firehose, which is great!\n\nFeedforward Networks\nCNN (convolution and pooling)\nVisualization of networks\nRNN, LSTM, & Embeddings\nTransformers, Deep Reinforcement Learning\nUnsupervised/Semi-supervised DL & GANs\n\nLectures\nThe lectures are a bit of a mixed bag. They mostly stay pretty high level, and you cover the material quickly over the semester. But... they don't go very deep and then later on you are expected to know that lower-level detail on the assignments/quizzes. The book is only okay at supplementing at this so you need to search out other material, like other school's lectures, to really understand the material. And then the Facebook (Meta now...) lectures are basically useless and too high level to be of use.\nAssignments\n\nAssignment 1: Building a NN from scratch, very good assignment to learn the basics.\nAssignment 2: Building a CNN from scratch and then using PyTorch, pretty good overall.\nAssignment 3: Visualizing CNNs, which is kind of fun, however, Gradescope doesn't have near enough tests to get full coverage and you have to compare your images to images in the PDF, which can seem close to your human eye but apparently aren't close enough and you'll get ducked points. In theory this is a great assignment, but they need to give you better tools to self-assess. It's removed in summers and almost worth taking in the summer just to skip it.\nAssignment 4: Making an RNN, LSTM, and transformer for NLP. I rather enjoyed it, but you really want a GPU to test this out and get your losses low enough.\n\nGraded Discussion\nOn the one hand you do review fairly interesting papers, on the other hand, the discussions don't add much and I'm pretty sure they just have an automated word counter for whether or not you finish this section.\nGroup Project\nThe group project is great in theory; however I feel you're not given enough time to do it justice. In BD4H you had over a month to focus on the project, whereas in DL, you have two weeks after the last assignment and quiz to focus just on the project. With assignments and quizzes you're kept constantly busy. On the plus side, the grading is very generous, perhaps too much so.\nQuizzes\nThe quizzes will make or break your grade. And honestly they feel kind of like bs. There are questions on the quizzes which feel like they come out of nowhere from the lectures and even the papers and unless you have a perfect knowledge of what was in all of those you will miss plenty of points. Earlier on they gave some good examples of computational questions on the quizzes, but later they don't at all. The TAs/Professor should do a better job given examples of what to expect and I feel they let the class down in this regard.\nEd Discussion\nOn the plus side, the TAs/Professor are very active on Ed, and quickly answer questions. It is possible to get extra credit on Ed by getting \"endorsed\" posts, which generally are best done via finding papers about SOTA techniques and/or making detailed notes to help other classmates.\nTips for Success:\n\nI highly recommend watching the CS231n (https://cs231n.stanford.edu/2017/syllabus.html) and EECS598 (https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/schedule.html) lectures from Stanford and UMichigan to supplement the course lectures (frankly I think they're better). Both EECS 598 and 7643 were based on CS231n originally, and 598 is taught by a former Stanford CS231n TA.\nTake copious notes of lectures, review the papers mentioned in lecture, post the notes to Ed, and make some quizlets to test yourself. Quiz materials can come from the lectures, the papers, or frankly related sources.\nBrush up on your calculus, especially with regards to partial differential equations and linear algebra. The first six chapters of Goodfellow's Deep Learning book are good to review and understand intimately.\nIf you can, get a good GPU or get used to working in Colab, they make the assignments and project far easier, especially A4.\n",
            rating: 3,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 27, 2021',
            semester: 'fall 2021',
            review: "This is the third course that I take, with AI4R and ML4T as the first two. I completed a M.S in CS already and have good understanding of ML and with some prior knowledge of DL (Completed part of DL Specialization by Andrew Ng.)\nOverview\nOverall, according to my experience, this is a fairly organized course, which covers a broad range of topics in DL such as Gradient Decent, CNN, Language Model, Semi-supervised learning, Deep RL and the advanced topics such as GAN. The lecture itself is very informative and you will get to know all these up-to-date topic and DL techniques.\nQuizzes\nQuizzes are demanding and require effort and study time to devote and really understand the concept, and first two quizzes includes more computational questions that requires you the calculate the Gradient Decent and input the number for it for example, the later quizzes are more on the conceptual questions but still entails computational questions. You need to study the detail concept covered in the lecture. For the first two quizzes, they provided TA tutorial and some samples questions and solutions which help you to get prepared a lot. The third and the fourth ones are quite difficult. And the rest three, usually, some students put a lot of relevant study materials and that help. In general, going through lecture slides and understand the concepts are necessary for combating quizzes.\nAssignments\nThe second assignment is the most challenging one as it requires you to implement backprop from scratch but still provided code template for doing that. The first one is easy as long as you understand the gradient decent. The first half of the course is more math heavy with gradient decent. The third assignment is a bit easier and very interesting to visualize the learning algorithm of CNN and implement style learning and transfer learning. The last one (4th) is challenging as well, but worth the time you put in as you will learn a lot about the transformers and machine translation. Overall, the assignments are well designed to cover broad range of topics too.\nFinal Project\nFinal project is a group project, and it depends on various factors such as finding good team members and choose the project that is feasible in  short period of time. Don't be too ambitious, but the rubric said they focus on you gain and learn DL techniques but not exactly care about if you succeed or fail to accomplish your project. Thus, it's okay to show not good experiment results or discuss your failure experiments as part of your project, but make sure to showcases your understanding through quantitative and qualitative analysis of your experiments.\nGraded discussion\nThis is an outstanding feature of this course and you will learn a number of recent research papers and discuss it in summary and comments with your classmates and learn from one another. I find it quite interesting and a lot learning from it.\nOA & Tutorial\nThe Prof. K was very dedicated and organized a lot of OA time with prof himself or tutorials with TA to explain difficult concepts, or overviews for the assignments. I would recommend to go through the overviews of the assignments OA as it will give you directions and save you a lot of time understand it. Other than this, FB Office hours or OA with AWS are accessible as AWS provides some free computing powers (GPU) to your final projects too.\nSummary\nIn summary, I personally had a good learning experience this semester and think I learn a lot from this course, and highly recommend this course if you want to learn DL. Meanwhile, I found the quizzes, assignments are challenging but worth the time you devoted to. I personally learnt a lot from them. The entire semester is busy, some weeks with quiz due only is a bit easier. But after the semester, I feel very accomplished personally.  Time spending on each week various,  weeks with assignment or final project due demanded more time, but weeks with quiz due are a bit easy but still needs the effort. Hope this help to others who are considering this course.",
            rating: 5,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 23, 2021',
            semester: 'fall 2021',
            review: "I loved this course. I learned so much, and so much of it plays right into my work in a computer vision startup where I work in product management in a role that lets me get my hands on our technology. This was my ninth course in OMSCS (btw I have not taken ML, officially a pre-req, but I didn't feel like I was missing any of the content not having taken ML first).\nThe coding components of the assignments are auto-graded, which I always prefer to non-autograded coding assignments. You can keep hacking away at at it until it passes, and there are plenty of people on EdStem or Slack to ask questions of when you're stuck, who have encountered that exact same error message as the one you're currently stuck on. The report components of the assignments were of the type where, if you answer the questions in the template, you get most if not all if the points. I missed 1 or 2 points out of 30 on some of the reports for submitting OK but not great answers. All in all, getting scores in the high 90%s on the projects isn't terribly difficult, but it requires doing the programming and then completing the report. That being said, I learned a fair amount about how pytorch works through these assignments, and I hope to keep that facility with pytorch up going forward.\nThe \"quizzes\" are definitely the most challenging part of the course grade-wise. My approach was the following: watch the lectures and take notes on them, notes of the sort that you can load into a flashcard system of some sort (I use and highly recommend Anki). Then watch the Stanford CS 231n or 224n lectures corresponding to the gatech lecture in question, and take notes on them as well. I found that the Stanford lectures came from a slightly different angle and that watching both gave me a much fuller understanding of the material. Armed with those flashcards, I would review them in the week prior to the quiz when out on a walk or at other times when I was waiting in line at the store or something. This approach resulted in quiz scores that were all closer to full marks than to the mean score. Oh, for the quizzes that had official study guides to go with them, definitely work through those, you will regret it if you don't.\nYour experience with the final project depends quite a bit on your teammates and the project you choose. We chose an ambitious project, which didn't really end up rejecting the null hypothesis in the end, but, we put together a complete report that was responsive to the rubric and we got full marks on it. Find teammates that you get along with and respect, and then, do your best to be a good teammate. There were definitely times when I felt like I wasn't pulling my weight, due to family or work obligations or whatever, which inspired me to try harder at other times to catch up and do what I could to move the project forward. At the end, each teammate fills out a form reviewing the effort put in by each of their teammates. If you are reviewed as \"never helping\", my understanding is that you could earn a 0 (for 20% of the grade) for the final project, despite the project getting a perfect score for the other teammates. Don't be a deadbeat!\nThe graded discussions were a new-to-me course mechanic, and I thought they were an interesting new way to get people to learn material via research papers. I preferred this approach to the one in AOS in which you have to summarize papers.\nI might as well chime in that I agree that the FB lectures weren't great. Could I have done better if I were in their shoes? I have no idea. I applaud the individuals at FB who stepped up and recorded lectures for our use, but I can't help but wonder if there might have been a way to better support those lecturers such that the final product were closer to the quality level of the rest of the lectures.\nOverall: Great course if you want to learn a lot about how DL works, as well as get experience with using PyTorch to build things.",
            rating: 5,
            difficulty: 3,
            workload: 12,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 21, 2021',
            semester: 'fall 2021',
            review: "This was my 6th course in OMSCS after:\nComp Photography, AI4R, Software Arch, CV, ML\nThis was my most difficult course so far, despite the fact that I've worked as a Data Scientist and have studied neural networks before.\nAssignments take a long time. Due to the hidden nature of the autograder, I spent hours and hours trying to debug insignificant and esoterric elements of the code.\nYou'll get to understand how to code in PyTorch more than you will actually understand some of the course content. This is the tradeoff of enrolling in an online program- the instructors must rely on autograder which means the students end up focusing on matching the results exactly with the expectation as opposed to studying and learning the course concepts.\nThe quizzes were very difficult. Even with watching all the lectures, taking notes, and doing the readings, I ended up with around average score on the quizzes which sometimes was down around 60%.\nMost of the assignments focus on Computer Vision applications which was disappointing. It was interesting but I would prefer a more common application since many ML/DL Engineers don't work in CV. I enjoyed the sections on NLP but felt the Deep RL section were unnecessary.\nThat being said, the material is very interesting and presented well. There is A LOT of material to learn, and instead of going deep into a few important concepts, this course prefers to go wide and touch lightly on many subjects within deep learning.",
            rating: 4,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 21, 2021',
            semester: 'fall 2021',
            review: "I liked the course. Got what I wanted from the course. I had taken RL and ML4T.\nEarly in the course, there were office hours on math background required for the course (matrix calculus, backprop etc). Watching these helped with initial quizzes and assignments.\nLectures from Prof. Kira are good and concise. However FB lectures are not organized well and most of them are bad. I feel they should be redone for future semesters. I used other sources to compensate for FB lectures.\nQuizzes contribute to 20% of the grade. They are difficult but are good for testing your understanding.\nI liked the assignments. They helped reinforce the learnings from lectures. However I felt fourth assignment could use some improvements in instructions.\nGraded discussions encourage to read some interesting research papers from past couple of years. They account for about 5% of the grade.\nAlthough we got a good score in the group project, I didn't have good experience with it. This depends on the team you end up with. To me, team members not attending scheduled meetings and leaving things to the end, was stressful. I would have liked if an additional assignment replaced the group project.\nThe grade distribution and absence of a curve requires one to do well on all the components.",
            rating: 4,
            difficulty: 3,
            workload: 13,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 20, 2021',
            semester: 'fall 2021',
            review: "Very nice course - probably going to be my favorite and the one I come back and refer to many many times in the future. The same deep fashion project I wasn't even able to properly load in the data, this course helped (or forced lol) me to complete it.  I can tell I visibly had some serious improvements in deep learning skills. The assignments are quite challenging, but I learned a lot by doing them. Understanding the assignments directly lead to the success of my final project, which pretty much saved me from getting a B.\nThe other reviews mentioning the drawbacks of this course, however, are true. The quizzes are hard and stressful, losing marks on them does add up at the end. The facebook lectures are very poor quality, I would say future improvements will hopefully remove them from making lectures, instead they will focus on providing guidance and mentorship on projects.\nOverall, this is a great course. Very much worth your money, and some hard work. You should give it a try",
            rating: 5,
            difficulty: 4,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 20, 2021',
            semester: 'fall 2021',
            review: 'This is a challenging class that covers a lot of ground. Like the other ML classes I have taken in the program so far (AI and ML) it is like drinking from a firehose. Deep Learning is one of the frontiers in terms of what ML has been able to achieve in recent years and is responsible for a lot of praise the field gets (e.g large language models like GPT3 and other popular advances in Computer Vision and NPL, which is something you will understand after this course). In this sense, this is an essential class to take if you are in the ML track or are seriously interested in ML.\nLittle bit about the class, as most others have said below, the first 1/2 of the class is really good. Be wared that it is math heavy but I feel like diving into the math really lets you absorb how deep neural networks are working. The second half gets progressively worse especially with the FB lectures. I think the class tries to cover too many difficult topics too quickly in the end. Hopefully they improve that in the future.\nThe 7 quizzes are difficult and can be a little annoying, especially given everything else you need to keep up with. I did like the exposure to all the really interesting research papers we had to read for this class and is one of my most valued take-aways from the class. Being able to read the most important DL papers on your own seems like an essential skill to keep up with this quickly growing field.\nHope you enjoy the class if you take it, prepare for a difficult ride and know that it will be worth it in the end. See you on the other side.',
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 20, 2021',
            semester: 'fall 2021',
            review: "Completely no quality control on Facebook lectures. Completely unfair for TA grading on Final projects. Just don't waste your time and money on this course and all other ML tracks.",
            rating: 1,
            difficulty: 3,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 19, 2021',
            semester: 'fall 2021',
            review: "I really enjoyed the content of this course, i think that Prof Kira is one of the best professors I've had!  The assignments were interesting, the quizzes were weirdly challenging (they did force you to really pay attention to the material, but with little to no guidance), and the final project was just okay.  I would have much rather had a couple of other homework assignments to cover all the topics.\nMy only complaint is the very poor quality of lectures by the Facebook team.  The biggest problem is that this class is one of the few, if the only one, that actually teaches NLP, and it was poorly covered by a few Facebook researchers who are good at reading off the slides, but not great at actually teaching.",
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 19, 2021',
            semester: 'fall 2021',
            review: "Class starts out strong but continues to get worse in every aspect. The lectures get worse, the assignments get worse and the quizzes are pointless.\nThe first two assignments are pretty good I felt like I learned something from them. The last two assignments are basically trying to figure out how to exactly implement the awful instructions that are provided and don't aid a lot in learning.\nThe quizzes are absolute garbage. At the beginning they aren't too bad but very focused on doing math exactly in the style taught. Then they just become useless trivia that in no way measures your understanding of the material.\nGroup projects don't work in this program and the teaching staff made absolutely no effort to try to make it work. The provided projects are very poorly described and there is no check in or follow ups or anything to ensure everyone in the group is capable of contributing.\nOhh and the graded discussions are just a waste of everyone's time\nOverall there is some good material in the class and then it is ruined by the worst structured class I've had in the program. The class structure needs to be done which is pretty pathetic given how new the class is",
            rating: 2,
            difficulty: 3,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 18, 2021',
            semester: 'fall 2021',
            review: "TLDR: Great course, demanding workload and conceptual difficulty. Roll with the punches (quizzes, FB lectures). You will learn a lot and feel like you have earned it.\nInstruction: As mentioned repeatedly below, Prof. Kira's lectures are great and communicate useful, interesting rules of thumb about deep learning. The FB lectures are not good - too high-level and hand-wavy especially relative to the degree of knowledge that is tested on the quizzes. The TAs (special shout-out to Farrukh, Alex and Sangeet) are excellent, patient, and ran high-quality tutorial sessions.\nAssignments: Hard & time-consuming, but worth the effort to get comfortable with architecutres. Best to come into the course feeling confident in Python and data structures. A4, relative to the others, was painful as the test harnesses weren't as well-constructed as the others and the instructions were less clear. Getting a subscription to Google Colab is basically a must for the portions where you have to tune PyTorch networks to find good hyperparameters. It might feel a bit painful at first, but I highly recommend writing up a hyperparameter tuning script to save down best configurations and running it overnight. I tuned all my models manually and that was more stress than it was worth.\nQuizzes: The complaints are mostly valid, lol. They are overfit to the lecture videos, so those that correspond to FB lectures are brutal. To minimize the damage you take from these, be overly-focused on every word said in the lecture videos (said, not just written on the slides).\nGroup project: It is what you make of it. I had an extremely good group. We attempted a novel architecture for our project which didn't perform how we had hoped, but I'm glad we gave it a shot. Probably would have been easier/smoother to try something more vanilla.\nIn summary, I highly recommend this course. Easily the best course I've taken so far (prev. CN, DBS, DVA, SDP) and the most worth the workload. I came away with a strong and crisp understanding of a lot of concepts within DL, and a better programmer to boot.",
            rating: 5,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 16, 2021',
            semester: 'fall 2021',
            review: "Good overall. Prof Kira's lectures are good. The FB ones, not so much. So you'll need to take the initiative to find other resources for that material (e.g., Fei-Fei Li's lectures). Note that you need to have some calculus and linear algebra background. If you don't, you're not going to survive the first couple of assignments. The assignments were fairly hard, but you learn a lot. If you've just been through a couple MOOCs on DL and think you understand NN's and backprop, trust me, you don't. Once you can get assignment 1 to work, you will. I see some people complaining about group projects, but if you form a group early and find smart, reliable people (there are plenty), it's not going to be an issue.",
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'November 25, 2021',
            semester: 'summer 2021',
            review: 'Strongly recommend to have an introductory understanding of deep learning. Need to implement NN from scratch. Forced to read papers and think about them.',
            rating: 5,
            difficulty: 5,
            workload: 37,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'November 22, 2021',
            semester: 'fall 2021',
            review: 'Update:\nI would say it was a bad experience in Fall 2021. The first half of this course went well. However, the last half was totally trash.\nAssignments:\nAssignments took 55% weights of your final grade.\nThere were four assignments. A1 and A2 were well-organised. As for A3 and A4, their instructions were ambiguous and should be clarified further. TAs graded the assignments leniently. But their feedbacks were always similar to “we did not expect that answer”.\nQuizzes:\nMost complaints were from quizzes. 7 quizzes took 20% of your score and had more multiple choices than before. As other comments said, some of them expected you understood the concepts that were only covered 2-3 seconds in the FB lectures but contributed 25% of one quiz. You worked hard on recitating lectures but usually achieved trivial or no improvements in the quizzes.\nGraded discussions & Final Project:\nI have no words about the Graded discussions and Final Project.\nI hope I could not take this course if there was a time machine :( . Standford cs231N and Umich EECS 598 are better choices if you want to learn deep learning. Most content (included assignments) of this course was copied from cs231n.\n11/22/2021, 3:01:09 PM:\nFB lectures are awful.\nAssignments are less organized. Instructions for Assignment4 are so ambiguous.\nGraded discussion is good. It helps us review some famous papers.\nThe most annoying things are 7 quizzes!!! If profs and TAs want to examine our learning outcome, please use mid/final exams! These quizzes are time-consuming and useless. They will determine your final grade. It is a discouraging thing.',
            rating: 1,
            difficulty: 3,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'November 21, 2021',
            semester: 'fall 2021',
            review: 'See "Difficulty" and "Workload".',
            rating: 4,
            difficulty: 5,
            workload: 16,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'November 21, 2021',
            semester: 'fall 2021',
            review: "I have very mixed feelings about this course. During the first half, I loved it. I feel the course is very well organized and the course materials are fantastic. However, starting from the sequence models, I think this course is absolutely trash, the FB lectures are disasters.\nI think the Deep Learning teaching is facing a very serious problem, in the sense that, people want more students to learn deep learning, so they reduce the difficulty and expand the topics to cover as much as they can. However, the quality is reduced dramatically.\n\n\nMathematically, this course started strong, and ended poorly. It has a good introduction on backpropagation and covers quite a bit about how to derive it.\n\n\nThe worst thing I don't like about this course, or maybe Deep Learning education in general, is that, they ignore the fact that Deep Learning course should serve as a starting point as students go on their deep learning application adventure. It's good that they focus on a lot of advancement in this field, and deep learning truly is constantly evolving. But by making deep learning course a memorization of all advanced methods is not a good idea, as least for me. Why should I remember each architecture without even knowing what is this? Like, what is ResNet, what is their advantage? This is becoming a literature review class. And for sequence models, it's so bad. I admire people from Facebook, they are super smart for sure. But consolidating their years of research in a 15 minutes lecture is beyond me. They even had a hard time explaining embeddings.... We have to read extra materials on YouTube or medium.com to learn it. Then what is the point of including this in this course.\n\n\nI saw many discussions about people wanting to have a natural language processing class. And maybe we should.\n\n\nIn summary, I think if we want to know more detailed stuff in Deep Learning, we should not take this class. But if this is the first class of your Deep Learning adventure, maybe it's OK. I really hope the Deep Learning educators treat this as a \"Computer Science Version of Calculus\". Not some research sharing, or research showcase. They are indeed useful, but not as something in this introduction course. Even worse, please do not quiz people with pros and cons of different architectures without letting people understand it.",
            rating: 2,
            difficulty: 3,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'September 8, 2021',
            semester: 'summer 2021',
            review: 'This is a very hard course. The first half focuses on MLP and CNN. You need to implement back propagation from scratch, then in the last assignment of CNN you’ll have a chance to work with pytorch. The second half course focuses on NLP. It’s a very busy course, you’ll have quizzes, assignments, readings almost every week. There is also a big project to replicate a paper, which is not easy also.\nTips for this course is to start everything early, especially for assignments. Use the piazza as a resource, since often people would share their solutions on it (without codes). Also the office hour is very helpful for quizzes and assignments.',
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 24, 2021',
            semester: 'fall 2020',
            review: 'First couple of lectures were really good and after that it looked like rushed and incoherent. This is the one of the few courses with TAs least helpful, may be because I took the course it was first offered in OMSCS',
            rating: 1,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 15, 2021',
            semester: 'summer 2021',
            review: 'I found the course extremely demanding but rewarding.\nI had some previous experience in Deep Learning through Udacity, but this course is a completely different ball game!\nThe syllabus is complete: it covers the basics, the math behind deep learning, ConvNets, sequential models/NLP, attention models, and a little of more advanced topics.\nI loved the assignments and the final group project. (Not so much all the quizzes and graded discussions.)\nBut I can say for sure my level of understanding and proficiency in Deep Learning is in a different level now, thanks to this course.\nIf you are serious about AI/Machine Learning/Deep Learning, this course is a must do!',
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 13, 2021',
            semester: 'summer 2021',
            review: "4th OMSCS class after RL, HDDA, and CN. Taking it over the summer, you miss a few sections on scalable training, deep RL, unsupervised/semi supervised learning, and generative models.\nYou also miss one of the four projects - you only do project 1, 2, and 4 in the summer session. Project 1 was about building a simple fully connected deep NN from scratch, using no ML libraries. Project 2 dealt with building a CNN from scratch also, and then using PyTorch to build a few CNNs and experiment with. Project 4 dealt with building language models using Pytorch: RNN, LSTM, Seq2Seq, and Transformer Architectures. The projects are worth 55% of your grade.\nI feel a lot of the learning comes from the projects, both practical in application, and theoretical from the fact that you do make it from scratch - admittedly, I did feel I didn't learn as much on project 4, however, since it is \"less\" from scratch; felt I was more following directions.\nThere were 5 quizzes for the summer session, closed book, proctored, untimed, 4% each, totalling 20% for quizzes - all multiple choice or \"enter a number\" (computation). They would cover lecture video content and reading - I did no reading, however. As everyone else has said, the lectures by the professor are impeccable, but the Facebook guest lecturers are pretty trash in comparison... they did do enough for me to get through the quizzes and assignment, however. The professor's lectures have a good balance of theory and practice in them, while the Facebook lecturers are pretty much like medium posts. The math in this class is very light, and they manage to teach the subject adequately from a CS practitioner's perspective.\nParticipation is 5% of your grade, which comes from 3 paper reading discussions - you basically pick a paper, read it, answer some questions posted on Canvas forum, and then have to have two replies to other peer's posts. Didn't enjoy this section at all, but I like the effort they made, as paper reading is invaluable. I felt maybe they could have made one of their projects a more paper-intensive task. I actually didn't read a single paper, just skimmed a few and tried to answer to the best of my ability.\nThere is also a 20% of your grade group project, and with all group projects, are only as good as your group - I luckily got a very good and enjoyable group, so I had a very good experience with it, especially it being my first group project.\nOverall, a must take for anybody who wants to learn deep learning! I do feel like RL is still the superior ML elective - much more math and paper writing in there. However, I've been trying to learn deep learning for so long, and this class finally taught me it adequately, so I am very grateful for that. My workload number is not very accurate, by the way.",
            rating: 4,
            difficulty: 3,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 9, 2021',
            semester: 'summer 2021',
            review: 'Before taking this course, 1) I had only used neural networks as black box models to do trivial analysis 2) was not familiar with PyTorch.\nThis course demanded a lot of hours for completing assignments. The assignments help you learn a lot and are very well set up. They require understanding of OOP in Python.\nThe exams are pretty hard, but they don’t hurt you as much because they carry less weightage.\nThere was something due every week and so the pace of this course is relentless. In the end, it was worth it because I came out of it learning a lot in a very short period of time.\nFeedback to instructors:\na)\tAt the start of the course, a lecture video is needed for calculating the computational graph for a 3-4 layer network. There was no full example shown and although there was an office hour session where this was covered, TA glanced over the crucial steps.\nb)\tNLP portion of the course did not have a good build-up of basics. As another review pointed out that there is really no good textbook. A ground up explanation of fundamental NLP would help students who are taking this course to learn from zero as opposed to being already experienced in the topic.',
            rating: 4,
            difficulty: 5,
            workload: 30,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 8, 2021',
            semester: 'summer 2021',
            review: "CS7643 Deep Learning - One Overhyped Class!\nDisclaimer - I took this class in summer (for the love of OMSCS, don't do this) and I ended up in a bad project team. So YMMV\nBackground - Non-CS bachelors; Comfortable with Python; Reasonable exposure to ML basics; Completed AI and CV so far\nGrade - A (98% in assignments, 80% in quizzes, 96% in project)\nIntroduction\nI have no idea why this course has so many rave reviews. It was not a terrible class (hence the dislike and not strong dislike), but nowhere near as good as some of the reviews suggested. The lectures are mediocre (don't even get me started on the Facebook lectures. That has a rant section of its own below), half of the assignments were pointless and the group project was just two weeks of frustration with absolutely no meaningful end result.\nIf you really want to get started with deep learning, I would strongly recommend looking at Dr.Andrew NG's Deep Learning specialization on Coursera. I ended up doing that in parallel with this course and it did pretty much everything better than this course. The lectures were miles better in quality and the assignments were easier but much more meaningful and it was about $700 cheaper. I apologize if I come across as promoting a different course here, but I was so disappointed that an MOOC can offer so much content that is better in quality than a GaTech course.\nHere is a section wise breakdown along with my ratings:\nLectures and Content (5/10)\nIn the beginning of the course, I was delighted that this course had its own dedicated lectures and slides and not some Udacity cut and stitch job. I enjoyed the first few (say 25%) of the lecture videos. I felt the professor - Dr.Zsolt Kira put in a lot of effort into introducing the fundamentals of deep learning. Basic neural network concepts, optimization, CNNs were all covered very well. The CNN architectures is where things began to fall apart for me. The lectures started to feel more rushed with a lot of diagrams thrown in but no proper context. (Dr. Andrew NG's specialization does a great job here and teaches you the intent behind each of the architectures with a lot of insights on the implementation). From here, it was downhill. The Faebook lectures start to take over and the whole experience is ruined. Also, the textbook was not great and I ended up returning it in 10 days.\nFacebook Lectures (0/10)\nOne of the reviewers used the term \"pedagogical disasters\" to describe these lectures. Those words could not have been more accurate. This is easily the worst aspect of the course for me. These guys may be world class software engineers and I respect them for that, but they should stay away from teaching for the rest of their lives. Every single video was beyond terrible and most of them go like this: Self introduction -> throw in some random technical term with absolutely no context -> brag about some cool stuff that Facebook did in this domain -> the end. Please Prof. Kira - get rid of these ASAP. These concepts are valuable and need to be taught well. Even if you provide links to YouTube videos on this content, I would be okay with that. \nAssignments (5/10)\nMassive disappointment. According to me, the assignments are the most important aspect of any course and can potentially save an otherwise bad course. Not here unfortunately.\n\nAssignment 1 - Really good. You build your own neural network from scratch, implement loss functions and backprop, visualize the learning, etc. Medium - hard in difficulty and I learnt a lot. Included a detailed write up section as well. In general for write-ups, as long as you explain your tuning process instead of just showing a massive table of hyperparameters, you should be fine.\nAssignment 2 - First part was building CNNs from scratch and was pretty brutal. But it was worth the slog. The second half was CNNs using PyTorch - good introduction to this library but nothing beyond that. The third part was class imbalance and focal loss. In theory this was interesting, but was not executed well in the assignment. The instructions were vague and we had no idea what end result they were expecting.\nAssignment 3 - Removed due to short summer semester. Why they removed a potentially valuable assignment and kept a useless group project is beyond me (same 2 week time frame for both!)\nAssignment 4 - Without doubt, the worst assignment I have ever worked on in any course so far. Had to build encoder-decoder type sequence models and transformers. It was setup in a terrible way without any introduction to what you were actually trying to achieve. 90% of it was wrestling with tensor dimensions and blindly trying to pass the autograder. I got 100/100 but learnt absolutely nothing.\n\nOverall, the assignments (A4 and most of A2) didn't teach me anything particularly useful or relevant to the real world. My advice is start early and keep learning expectations to a bare minimum\nGraded discussions (7/10)\nThere were three of these - You essentially read cutting edge DL papers, answer some subjective questions and respond to others' posts on Piazza. The papers were great, the questions were great, but the responding to others part was forced and you just end up making generic comments. On the plus side, they were graded leniently.\nGroup project (2/10)\nAbsolutely hated it. I really wish OMSCS gets rid of all group projects. I just don't see the value of these in a setting where most of the people are working professionals and are in different timezones. We worked on this project for about 10 days. Technically, we had about a month, but in the first two weeks, there were a gazillion other deliverables. I am pretty sure most of the folks spent under two weeks on the project. (The longer semesters should definitely be better in this regard) The project itself was weak - pick some preprocessed dataset, tune some really basic models and write 6 pages on how we changed the world. Absolutely nothing meaningful was achieved. I don't think it is possible to do a research paper type DL project in 10 days unless all team members are already proficient in this field. To make this worse, I ended up in a bad team - one person who didn't bother to review the work others had done and suggested last minute changes to everything and another person who hardly showed up to meetings or did anything valuable. I am not even sure if the peer reviews are factored into grading. Overall, a hugely frustrating experience.\nQuizzes (10/10)\nThe lone savior! This is the most challenging part in this course and can make or break your grades. There are five proctored quizzes (longer semesters have 11 I think) that are a mix of conceptual and computational questions. They are hard and you really need to understand the content well to get through them. I really liked the quizzes and felt a sense of accomplishment when I did well on these. Also, they were my main motivation to slog through some of the god awful lecture videos.\nGrading (8/10)\nAssignment grading was fair. Discussions and projects are graded leniently. Pay attention to the assignment write-ups which carry about 30% weight. You can potentially lose points here. The grading process is extremely slow and this creates unnecessary anxiety. Grades for Assignment 2 and 4, Discussions 2 and 3 were all released only in the last week. Overall, if you do the assignments, discussions, project well and somehow manage 60% in the quizzes (which is not easy), you have a shot at an A.\nTA support/Office hours/Slack (8/10)\nThe professor was attentive and held office hours. Thank you sir! (However, based on past reviews, looks like he was more involved during the longer semesters). TA office hour support was stellar during the first few weeks and was helpful to get through assignment 1 and quiz 1. But then, it dropped off steeply. The later sessions were not very helpful. Piazza was a dumpster fire especially during assignment 4. TAs just couldn't handle the chaos and there were numerous questions left unresolved. Surprisingly, the slack channel which is usually bustling with activity was relatively quiet. A shoutout to TA Farrukh Rahman who was active and remained extremely polite throughout the semester.\nNote on Workload\nThis is a high workload class (20 hours or more per week). Without exposure to ML and Python, this will be borderline unmanageable. I strongly recommend you take this course in the longer semesters. This is definitely not a summer course. You end up doing 75% of the work in 67% of the time. It is a constant barrage of deliverables week after week. It will probably be similar in spring or fall but you would at least have more time to work on the project.\nOverall (5/10)\nIn short, this course was a disappointment. Unfortunately, it is not a case where there are a few rough edges and will be eventually sorted out. It needs significant rework - assignments need to be made more meaningful, Facebook content needs to be redone, project needs to have a better definition and scope, etc.\nTake this course if you want CS7643 - Deep Learning to show up on your transcript or if it helps you meet your specialization requirements. But if you want to actually learn deep learning, look elsewhere. There are tons of good resources online.",
            rating: 2,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 7, 2021',
            semester: 'summer 2021',
            review: "TL;DR its a good course and you'll learn a lot but take the course in the fall or spring. its too much work in the summer\nThis course was a mess in the summer. They did not scale down the workload properly. In ~67% of the time we needed to do ~75% of the quizzes, discussions and assignments and 100% of the final project.\nThe assignments are the best part and I learned a lot. They were time consuming but manageable (this is a theme in OMSCS. most assignments are difficult but manageable if started early and a disaster if started late). I learned a lot while doing the assignments.\nThe final project was fine. I would have preferred a solo project but my team was pretty good and I learned a lot during it.\nThe quizzes were OK. There were some interesting things we needed to study in preparation for them (computation graphs, parameter/dimensions calculation) but some questions felt like trivia and we had to memorize equations and I don't see the point of memorizing these kinds of equations in order to do arithmetic at this level.\nThe discussions were a waste of time. The papers were interesting to read but no real discussion was facilitated. People simply left generic comments probably because we were all swamped with other work.\nI would recommend this course in the fall or spring but not summer. That should make the pacing of this course more manageable. It felt like we had two things due every week and coupled with summer vacations it was this constant feeling of dread of due dates.",
            rating: 4,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 6, 2021',
            semester: 'summer 2021',
            review: "I'm writing this review from the perspective of someone who took it in the Summer, so it may not be indicative of your experience in other semesters.\nPros:\n\nProfessor shows concern about improving the class.\nThere is a level of math understanding required that is higher than in other CS courses in the program\nThe assignments are useful to build intuition by coding low-level operations manually. The only one that I didn't enjoy was Assignment 4 where I finished but felt I didn't understand transformers completely.\nDeep learning is still cool I guess.\nQuizzes are good, but they rely too much on the lectures/slides and some questions resemble trivia at times.\n\nCons:\n\nThe Facebook lectures are TERRIBLE and there are many (roughly half) by them. Each one is taught by a different researcher from Facebook, but the common thing is that they all suck at teaching or don't care about the quality of their videos. They are overly short, have no build up (they often start speaking about concepts at a deep level without properly introducing them to the students), and are mostly inferior compared to even random Medium posts at explaining similar topics. Most of these lectures were clearly based on Stanford's CS231 and CS224n content, with many of the slides straight up copied from it, and it would be wiser to seek this original content instead as they're available for free on the internet.\nSummer had the last module, the most advanced and interesting, as optional. While the lectures were still available, there was no quiz to help test your knowledge and to act as incentive to walk through the lectures. Less content overall compared to the full semesters was a let down.\nProfessor was mostly absent this term. He held weekly office hours and made sporadic appearances on Piazza, but based on past reviews I believe he was less involved in this semester.\nPersonally, I found many questions on Piazza going unanswered for very long compared to other classes. And the students/instructor answers were often insufficient and lacked depth.\nI didn't find the office hours by the TAs very helpful. They felt similar to the Facebook's lectures, as in the TA didn't seem comfortable with explaining the material in a structured, pedagogic way.\nThe book could have been explored in more depth. Particularly, I felt the mathematical rigor present in the book wasn't explored in the classroom activities.\n",
            rating: 3,
            difficulty: 2,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'August 6, 2021',
            semester: 'summer 2021',
            review: "Prof Kira had some good lectures and was active on Piazza.\nTA and student quality of responses on Piazza was pretty lacking though. Response time on detailed and thoughtful questions was pretty slow with not always the most insightful reply.\nFacebook lectures were lacking in depth. Ex. they talked about beam search, but never mentioned length normalization, then the quiz had a question about length normalization in it. Having already done Andrew Ng's courses this wasn't a problem personally, but be warned the lectures are not all you need. I recommend prepping for the course by doing those.\nI liked the quizzes, they actually tested if you truly had a grasp on the material. The above about length normalization is an example of why people may not like the quizzes. However, if you stop at lectures you're hurting yourself. Papers, the book, and some further self study are crucial if you want to do DL for real. You get out what you put in. For example, the book had a great section on the loss function surface for RNN/LSTM type models and mathematically proving why gradient clipping was needed. That next level of understanding will take you a long way at work, since DL is sensitive and missing details will be crushing. They give a very generous study guide on what areas to focus on for the quizzes.\nFor the project, pay attention to their request of the effort of one assignment per person for the project. DL is fun but takes time to get to the cutting edge stuff, especially if its a newer subject for you. A lot of teams set out to change the world and ended up changing scope. The example projects are designed to push you but focus on quality of experimentation and analysis over how fancy your project is.\nFor my background I have a couple years of experience working as an MLE.\n\nAssignment 1: 8 hours, straightforward calc\nAssignment 2: 20 hours, CNN backprop takes a while to think through\nAssignment 4: 25 hours, the random seed initialization to get past the LSTM auto grader was a pain\nProject: 30 hours, group was meh\n\nGrades:\n\nQuizzes 90%\nAssignments 99%\nProject 97%\n\nGood class, not as incredible as others may say, a good balance of work.",
            rating: 4,
            difficulty: 3,
            workload: 12,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'June 3, 2021',
            semester: 'summer 2021',
            review: "First time this course is being done in the Summer. This is my 6th class in the program and has had easily the fastest ramp up in work. This will be the end of the third week and given that I have a full-time job, I feel like I'm way behind. For all of you taking this in the Summer, I wish you good luck, god knows I will need it.\n\nHave now finished two quizzes and the first assignment. First quiz went well but relied on being comfortable with the calculus. Second quiz was rough. I spent a lot of time going over lectures and I felt like I understood the material well. Ended up feeling like the quiz was just trying to trick people and did not reflect the lectures well on several of the questions.\nAssignment 1 took a lot of work. Definitely needs a couple weekends on it at least. The code we were given was good, I am happy with how the assignment was built. However, the guidance within the pdf and the comments in code left a lot to be desired. I would say some of the comments in code were even misleading in a couple spots.\nLastly, the lectures are passable. Definitely near the bottom relative to the other classes I have taken. The audio quality is poor and can be distracting. I don't think the material covered is divided into all the lectures very well. That is, an individual lecture is probably fine, but I usually was left thinking that it was not connected very well to the rest of the content. And lastly, it drives me crazy that there is no pointer that is used for me to follow where in the slide I should be looking. Often times there is a lot of content in the slide and I have no idea where I should be looking. I had to rewatch lectures just from trying to decipher when I should look at what.\n\nI now have just the group project left to do. I've hated this class. The subject matter is super interesting and that's helped me get through the semester, but I've hated this class. If you would ever need to know DL, then take it. But with a full-time job that is about 50hrs/week, this class has added a high level of stress to my life this summer. In particular there are always overlapping deadlines. For example, you might have an assignment (worth 20%) due next Sunday so you'd like to start this (previous) weekend, or you can study for the quiz which is due this Sunday which is only worth 4%. I always ended up studying the whole weekend and that barely got me to the average Quiz grades. And then the next week was an absolute hell trying to work on the assignment. I always felt behind and so it's been 3 months of stress. The subject matter alone is not 'Very Hard' but I am rating it as such because it has been the hardest class I have taken so far after two years in the program, and because consequently it has had the worst impact on my day-to-day life.",
            rating: 1,
            difficulty: 5,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 16, 2021',
            semester: 'spring 2021',
            review: "This course is a must-take for anyone doing the ML specialization. It gives a broad overview of many of the deep learning techniques currently being used in industry and research. It is overall well-taught and the material is fascinating.\nPros:\n-Professor Kira made himself very available with weekly office hours and a lot of support during the semester. He was clearly very invested in his students' learning outcomes.\n-The combination of programming assignments (building neural network components from scratch in assignments 1-2 and using PyTorch modules for assignments 2-4), quizzes, final project, and research paper discussions helped me understand and retain a lot of the material from a few different perspectives.\n-While the quizzes covered a lot of material and really demanded a strong understanding, each one was accompanied by a study guide that included most of the major topics to focus on.\n-Professor Kira is very open to feedback and is working on continuously improving the course.\nCons/Areas for improvement:\n-There was some uninteresting ambiguity in one of the assignments. Sometimes a specific function had to be used to match the expected output or a neural network architecture was not clearly described.\n-The lectures co-taught by Facebook employees had inconsistent quality and depth of coverage.\nThings to be aware of:\n-The course hits the ground running and there is a lot of material to wrap one's head around in the first month.",
            rating: 5,
            difficulty: 4,
            workload: 13,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 15, 2021',
            semester: 'spring 2021',
            review: 'This course is very similar to CV, especially the assignments and final project, but it is easier and takes less time than CV.\nEvery assignment is about implementing some DL methods (e.g. backprop, CNN, RNN, Attention) by filling some unimplemented functions. Local test code and an online auto grader are provided. There is also a write-up portion but some brief explanations are enough.\nQuizzes are the only difficult part of the course. Average was below 80%. The questions really test your solid understanding of the concepts covered.\nGroup project may sound challenging at first, but it is graded very leniently. As long as you cover all the points in the provided rubrics, you will get most of the scores. Report seems more important than code and experiments.\nOverall an easy-A and light course.',
            rating: 4,
            difficulty: 2,
            workload: 10,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 10, 2021',
            semester: 'spring 2021',
            review: "The course content is very good overall. I suppose I should classify it a survey course. If you have previous experience with deep neural nets, as I did from CV, then this adds to that knowledge pretty effectively, but frankly it isn't a very challenging course.\nThere are very many activities to keep up with (several office hours per week, lectures, readings, graded discussions, assignments, quizzes, final project). I didn't find anything in particular very difficult, but it is a little overwhelming scheduling-wise. Of all the graded assignments I found the (proctored) quizzes to be the trickiest to do well.\nAssignments are quite easy compared to other classes (as I remember, they are mostly filling in TODO sections with pytorch code), especially if you have numpy or pytorch experience (since pytorch operations are very similar to numpy), and the assignments are unit tested, so you know at the time of submission what your graded score will be (except possibly by some fluke). Some of the tests within the notebooks they provide were a little lacking in coverage, but it was only a small difficulty. Written analysis portions of the assignments are very easy compared to e.g. ML.\nThe final project is a group project, with all of the potential pitfalls, but for me it was my favorite part because it required the most coding, the most time, and was the biggest challenge of the course.\nIf you are thinking of also taking CV, I can say that I took CV first and did the CNN (individual) final project in that course, so I was well prepared for this class. Taking them in the opposite order could also work well, I'd only be guessing. I can say there is some overlap.",
            rating: 4,
            difficulty: 3,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 10, 2021',
            semester: 'spring 2021',
            review: "tl;dr I'd give this course 12/10 and you should definitely take it.\nI'll be real, I took a few days to recover before writing this up. Sure, I worked right up to the last hours at the end (earlier part of the course is easier to time manage), and, yeah, I and my teammates had a hard time being available for each other and keeping up with our individual workloads in the course (alongside FT jobs and late pandemic burnout), but this is hands-down one of the very best classes I've taken. The workload is constant but well delineated and hours put in will almost always translate to progress. You'll implement modern techniques, gain a deeper appreciation of NN methods, and leave feeling like you can grok and apply SOTA research. This is a rapidly changing field, but I have little doubt that Prof. Kira will keep this course updated with meaningful developments, if not in directly in lecture than in other aspects of the course. If you have experience with multivariate calculus (even if you're rusty) and a broad understanding of ML algorithms and challenges, you should take this course. I personally don't think a GPU is required in this class, depending on your ambition for the final project. I did all of it (even the project) on local compute, and the GPU available on colab would be enough for the assignments (which, again, you can run on CPU just fine, it's just faster to tune on GPU).\n1] Quizzes: If people have a gripe about an aspect of this course, it's likely the quizzes.  I think a lot of that is the mental association we have with the word \"quiz\"; unlike the low-stakes high-level knowledge that word evokes these are more akin to mini-exams. Because of the rest of the coursework is graded quite fairly (bordering on generously), this 20% is in practice the differentiator of your letter grade.  I personally found them fair but challenging; they do require you to have thought through the implications and meaning of topics which sometimes explicitly spelled out for you in lecture and sometimes something you need to stop and think about on your own. I found that, provided I stopped the lectures frequently and really thought about what was just said or explained, I did great on the quizzes (YMMV). There was only one quiz that felt a bit punishing in that it had some \"chained\" calculation questions (so if you got the first one wrong you'd actually get 4 wrong), but that was an outlier. Early on, TAs ran dedicated OH and made preparation worksheets for the calculation questions.\n2] Assignments: The current 4 assignments are building your own NN (including differentiation for  gradient descent), building your own CNN then implementing on in PyTorch, visualization of features and style transfer, and building RNN, LSTM, and Transformer solutions for NLP. Code is autograded (unlimited submissions) and most of the points; for the written part just make sure you actually offer an explanation of why you think you got what you got. Instructions are generally really clear (and there are copious office hours if you're stuck). The only assignment that I think needs a little tweaking is the final one;  it felt a bit more high level vs. the more granular nature of the others.\n3] Graded Discussion: This might feel like busywork to some, and I won't argue with that much. I will say that it was a prompt to read and think about papers that either extended the lecture materials or were a glimpse into late-breaking developments. It feels like a good part to keep in the course as a way to explore new findings or techniques, and while the \"conversation\" can feel a little artificial given the length rubrics for a substantive contribution, it was interesting to see how other students interpreted and responded to the papers.\n4] Project: Yes, take this class even though there's a group project. If you're the type that wants to get a paper out of a class, you can go for it. If you want to be ambitious and play with some Facebook problems, you can do that in this course. Everyone else, you're OK, too. People of the first two types should try and find a group by the halfway point of the class and start working, because the rest of the course keeps up the pace and those few weeks at the end aren't as much time as you may need; there were occasional glimpses of data accessibility issues and group squabbles on Piazza. Everyone else, it's OK if you just find a niche part of DL that extends the ideas in the class (that's what we did, and I was grateful our project was so contained and could run on local compute). Remember, the two biggest issues you're going to have with your project are sourcing your data and compute, so brainstorm with those constraints in mind.\n5] Office Hours: This part really shines. There are weekly OH with Prof. Kira, whose passion for the topic and commitment to students is beyond evident, if you want to get his take on recent DL developments. There were also multiple OH with Facebook engineers, which is just a phenomenal opportunity (even if I found them a little wan). And we're not done; the TAs ran recurring OH 6 days a week, some days even had multiple time slots. During some of these they'll do breakout sessions and offer you a code review if you're truly stuck (you can also get code review via private piazza post). Compared to most courses in the program, this course's OH are an embarrassment of riches. The TA presence is outstanding, and so is Prof. Kira's. Everyone is genuinely very helpful and positive.",
            rating: 5,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 10, 2021',
            semester: 'spring 2021',
            review: "I loved this class. Hard, but doable, which means the instruction and assignments were on point relative to the rest of this program. For reference, I got an A, am almost done with OMSCS, and have extensive ML experience from work.\nNotes\n\nTo get an A, finish the assignments and find a good group for the project.\nI averaged 60-70% on quizzes just because they personally took way too much effort to study for and I wanted to use my time elsewhere, so I just watched the lectures once after the first few. My friends who did better read the papers and external materials thoroughly.\nAssignments are the best part of this course aside from a few \"I have no idea what's going on\" moments and having to lean hard into slack / Piazza for direction. They really teach you modern deep learning and the PyTorch parts were so much fun\nUse Google Colab. Yes, you'll need a GPU, but Colab will get you through all assignments + Pro is only $10/mo if you need it for the final project. My group ended up using Pro which has a 3 parallel notebooks and a 24hr consecutive limit to GPUs and got everything needed done by splitting up the training.\n\nFinal thoughts: if you're on the ML track, this class might seem like another hard class after ML, but is completely worth it. If not, but are interested in ML, take this over anything else.",
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 10, 2021',
            semester: 'spring 2021',
            review: 'I like the assignments. The concepts are developed from ground up. TAs are very helpful. The link shared on by the colleagues helped to understand challenging concepts from different dimensions.',
            rating: 5,
            difficulty: 4,
            workload: 22,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 10, 2021',
            semester: 'spring 2021',
            review: "Deep learning is one of the best and most useful courses I've taken via OMSCS.\nThis class will teach you what deep learning is, how it works, and how to use it in the real world. If you are serious about going into deep learning, this is an excellent way to get your foot in the door. I'm pretty sure I could tackle some compelling Kaggle problems at this point.\nThinks to Know\n\n\nThis class requires a good deal of math in the beginning. If you aren't fresh on calculus and linear algebra, be prepared to spend a lot of time in the first ~5 weeks grinding. You don't need to know all the math to succeed on the homework/quizzes, but you'll be severely limiting how much deep learning intuition you'll get out of the class. I highly recommend spending the time on the math early and often to both make your life easier and improve your learning outcome.\n\n\nThis class requires a GPU. I'll repeat that. THIS CLASS REQUIRES A GPU. While you can technically finish this course without one (I did) and get a good grade, it is way more painful and you won't learn as much. Instead of spending time doing deep learning you'll be spending time pushing your code to expensive AWS instances or wrangling with GCP/Google Colab environments. If you're like me, and you are in this program to learn as much as you can, spend the money. Buy a 20x or 30x series GPU (ideally 2080+). It's 100% worth it and you can sell it after if you don't want it.\nNOTE: DO NOT BUY AMD. The ecosystem for deep learning is based around NVIDIA GPUs. Don't try to go against the herd, it's not worth it. If you're reading this in 2023 it might be different, but if it's 2021 for you it's 20x/30x series NVIDIA or bust.\nTl;DR: Go buy a desktop on which you'd play Crysis at full settings.\n\n\nThe final group project is your time to shine. The last ~5 weeks of the class is a final project. We had no final for this class (not sure if that will change), so it's really ~5 weeks of uninterrupted group project. Pick a compelling, challenging problem and be prepared to bang your head on it learning how Pytorch works. If you use this project time wisely, you will know how to train a \"real world\" DL model.\n\n\nPros\n\nAs stated above: you will know what deep learning is, why it works, and how to use it.\nProfessor is awesome, TAs are great, course pacing is good, assignments are good, quizzes force you to study, and the final project is well-structured.\n\nCons\n\n\nFacebook content is weak. Compared to the professor-generated lecture content, the Facebook lectures are pedagogical disasters. Hopefully this content gets removed or re-recorded, but you'll just have to suffer through it for now. Fortunately there isn't too much of it and nearly all the critical lecture material is produced by the professor (and is therefore awesome).\n\n\nQuizzes are brutal. I put this here but it's not really a con: you will be forced to thoroughly review the concepts of the course frequently, which is a good thing. However, be prepared for some pain. Study early and take good notes.\n\n\nThe textbook is (IMO) quite poor. It's either too meandering or too difficult for introductory deep learning students. I think there just isn't a good deep learning textbook yet, which is why the course textbook is so poor. Hopefully this will change soon, but in the meantime don't feel too bad if you can't figure out what the textbook is saying. The professor will clarify on Piazza exactly which sub-sections/sub-chapters to read. I recommend waiting for that post and only reading those subsections (but read them thoroughly).\n\n\nOverall, this is a fantastic course and you should take it if you can.\nTL;DR: Take this class, BUY A GPU BEFORE CLASS STARTS, and don't fall behind.",
            rating: 4,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 8, 2021',
            semester: 'spring 2021',
            review: "Even after ML and RL, this course was not easy, but worth it.  I liked it overall but found a few things annoying.  The annoying parts:\n\nProjects could have had better descriptions and instructions.  It was more like guess what they want to pass unit tests (some of which were flaky).  Too much time was spent on guessing and googling\nLectures by FB were mostly crap.  One I couldn't understand half of what the girl from FB was saying (very strong accent).  I have nothing against accents (I have one myself) but you have to think of the wider audience when the speaker is halfway unintelligible.\nReinforcement Learning lectures by a grad student = WTF.  I took RL class and it was still too hard to follow.  They squeezed the whole course into 40 min and expected people to just follow it.\nFinal project..  Very little guidance, difficult to pick just the right amount of work to split between multiple people.  I was VERY lucky with my group (smart, hard-working, and helpful) but this could've gone bad as well.  I was just lucky\n\nThe good parts:\n\nLectures by prof were mostly great\nThe TA's held very frequent Office Hours\nOne of the most responsive and attentive profs in the program.  First time in my 10 classes where the prof held weekly Office Hours as well.  And those were good, whenever I could attend.\nYou can learn a lot.  Now I want to continue with some of the areas independently.  It's still a high-level overview of many areas but they are a recent development in the field, some SOTA.\n\nOverall, this course can be GREAT.  Just drop FB lectures and group Final Project.\nEdit: Final Project is graded fairly leniently.  As long as you put some effort, do background research and explain your experiments you are likely going to do well.",
            rating: 4,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 8, 2021',
            semester: 'spring 2021',
            review: "Final course in the program which is also the one I spent most time vs. other courses (had BDH, CV, ML, AI, DVA, ML4T, AI4R, RL, GA). There's something due almost every weekend e.g. 4 coding assignments, 7 quizzes, 4 paper reading/discussions, and 1 final project which easily takes 3 weeks of dedicated efforts. So be prepared for this especially if you have a demanding job and family/kids.\nOn the other hand, I have to say that I learnt a lot from this course. I have much deep understanding on how back propagation works and how CNN learn its filters. I think the reason is that you really get the opportunity to get hands on exp to write the actual code that's the core of TensorFlow or PyTorch (of course it's toy size coding here...). TBH this is a must have if you are or want to be a ML practitioner, or you're on ML track. I felt lucky that they open this course right on time so I had the chance to take it.\nFor sure there're things to improve, e.g. course videos are too shallow and sometimes can be hard to follow due to lack of detailed description, the FB office hours and content can be more helpful if they include more prepared, detailed discussion of a topic and not just answering questions. Also I wish the final projects provided by FB can have more help available on troubleshooting their existing code/libs (we spent 2 weeks just to get their 'base' code run...). But overall a great course if you manage your expectation/workload well.",
            rating: 4,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 7, 2021',
            semester: 'spring 2021',
            review: "Definitely one of the better courses in the program. Content is current and for the most part well presented (see below). Assignments are fairly involved, but plenty of time is given for them. If you can prepare for the assignments by watching the lectures and finish early, there will be some weeks where you won't have that much to do, unless you decide to do all the optional reading, which gives some opportunities to breathe for a bit throughout the semester. Compared to the ML course, I liked this one a lot more, and the workload is slightly less I'd say (because you aren't spending your entire weekends writing reports). My only complaint would be that some of the Facebook lectures are pretty weak (it depends on the person that prepared them; there are 5-6 different Facebook lecturers). I didn't get much out of at least a few of them, either because they didn't go into much depth, or because they assumed too much background knowledge and/or glossed over stuff too fast. Wish the instructor would re-do some of these lectures, or at least supplement them a bit so that they'd be easier to follow. His lectures were much better.",
            rating: 4,
            difficulty: 3,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'May 1, 2021',
            semester: 'fall 2021',
            review: "Great class overall. The assignments still need some work - I learned to start slightly later than I would have liked to, just so some other students can iron out the bugs (thanks to all you trailblazers!). They are mostly auto-graded, and the report section is, in my experience, graded fairly leniently.\nLecture quality varies depending on topics. I believe (hope) that they are also improving that aspect as the class matures.\nGroup project is kind of an annoyance. I did end up with mostly fine group members but all the common issues that usually comes with group projects still apply - nothing really specific about this course.\nQuality of quizzes also varies quite a bit - they're probably still figuring things out as well. Professor will post a thread about the main topics covered by the up-coming quiz, which, again, varies in helpfulness.\nAll in all, a great course but still in the making. Follow the schedule, turn in your assignments, and you should be fine!",
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'April 23, 2021',
            semester: 'spring 2021',
            review: "An OMSA Student Perspective\nI took this course in Spring 2021, the first semester that it was offered to OMSA students. I am part of the OMSA program and don't come from a direct computer science background.\nSummary\nI highly recommend this course to anyone that is part of the computational analytics track! It is very thorough and detailed, but also very difficult. If you actually want to learn and understand the material, it will take a lot of time out of your day. It teaches you the fundamental DL skills, and also the basics of state-of-the-art technologies. It was the most challenging course that I've taken in my life, but also one of the coolest and most rewarding. I feel that this course has taught me all the skills that are needed to keep up-to-date in the ever changing field that is Deep Learning.\nOverview\nThis course was divided into 4 parts: Assignments, Quizzes, Participation, and a final project. The lectures were like drinking through a firehose, but I wanted to learn everything and it all seemed important.\nDr. Zsolt is a great prof. His lectures weren't too long, and everything that was taught was things that I wanted to learn. He was the most active professor that I've seen on Piazza in the entire program. The TA team was hit and miss, but he was always there to answer questions if it was needed.\nAssignments\nThere are 4 assignments and are worth 55% of the final grade in total. They are very time-consuming, but you are proud of your work after they are done. They involve everything from manually programming a simple CNN, to using PyTorch for language prediction. They capture the fundamentals and teach you the basics of Computer Vision and Natural Language Processing. I found they captured both theoretical understanding and practical application very well.\nThe assignments are very programming-heavy. It is important to understand OOP in Python going into the course. I feel that if you come from a computer science background, the assignments would be much less time-consuming, but having to learn programming methods and practices is a skill that I will carry forward throughout my career.\nSome parts of some assignments are autograded, but I never had any issues with this. Some reports need to be submitted, but they are as simple as copying a photo or table into a PowerPoint slide template - no LaTeX or unnecessary explaining required.\nMany of the projects involve PyTorch. This benefits strongly from having access to an Nvidia GPU. I have an AMD GPU that is not compatible with PyTorch which was a real bummer (there are workarounds if your AMD GPU is Vega, but support for NAVI doesn't exist yet). The workaround is to use Google Collab, which is a free service that utilizes cloud GPU processing power. It's a little more clunky than having a local GPU, but it certainly does the job.\nThere was an assignment that would only pass local tests if you had an Intel CPU. I suspect that AMD CPUs will be supported in future iterations of the course.\nQuizzes\nThere were 7 quizzes, worth 20% of the final grade. I did not like the quizzes at all. They were highly theoretical that tested your comprehension ability rather than your understanding of the material. They were proctored, closed-book, closed internet and you couldn't use your calculator. There were both theoretical questions and calculations. The quizzes were very difficult and did not seem to fit the theme of the course. They tested both lecture and reading material.\nParticipation\n4 graded discussions counted for 5% of the final grade. These involve reading selected research papers, summarizing them and answering some questions, and then replying to other people's summaries and answers.\nOriginally, I did not like these at all, but as I did more of them, I grew to like them. They are an opportunity to learn state-of-the-art technologies, and since Deep Learning is such a dynamic field and a very active domain of research, I think that this is an important part of the class. In this field, it is important to know how to learn about new technologies.\nProject\nAt the time of writing, the project is currently in progress. It seems like any other project in this program - form a group, write a proposal, submit a report. The project guidelines are quite general and it is a fun and exciting way to apply your ideas or explore new ones.",
            rating: 5,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'April 21, 2021',
            semester: 'spring 2021',
            review: "This is a great class, and anyone who is interested in Machine Learning should really consider taking it. It's very practical and hands-on. It is very math-heavy, especially in the beginning. Hard to imagine a more relevant machine learning topic today.\nTo prepare...brush up on your matrix calculus skills and check that you have some basic ML skills. Although the requirements say you should have taken ML, I did not. I did take ML4T and AI before though, and this plenty. I didn't feel left out at really any point.\nThe lectures by Dr. Kira are excellent, top-notch. The ones by Facebook are...varying in quality. Some are good, but I feel like Dr. Kira could do a much better job. Overall though, definitely some of the best lectures.\nThe assignments are like any other auto-graded class. You have a series of steps that build on one another, you pass or you fail, iterate, yada-yada. Most of the assignments have good unit tests that give you that warm feeling of getting a good grade. The only thing is there is a written\nreport you have to do which is about ~30% of the assignment grade, but it's not bad. (Definitely not like those dreaded CP reports)\nThere are discussions in the course, but I actually enjoyed the papers they centered around. These will not take much of your time.\nI do have a few complaints so far. One is that there is some roughness-around-the-edges. I knew that going in but it has actually cost me several days of work only to figure out the instructions of one of the assignments were wrong. In this case, it pays NOT to finish assignments early, or else you will fall prey to this problem. (Ahem...Focal Loss).\nI also think more of these assignments should be focused on some of the applications rather than designing the networks from scratch. There is a lot of “create this from scratch and struggle through it” and then “muahaha, see how it only takes two lines of code with PyTorch”. Maybe it would be better to spend that time doing some interesting things instead that we can walk away with.\nAlso, if they made the assignments a little smaller, they could squeeze another one in. For example, at the end there are modules on RL and Unsupervised Learning. The RL one was so briefly done by the Facebook lecturer, it doesn’t do the subject any justice.\nAnother is that the quizzes can be worded strangely and in such a way that studying any amount of time will not fix it. I don't know how to describe it, but it is like some of the questions are designed to trick you. They seem to me like they have been designed by someone who really has a lot of experience with DL and forgets we are students still learning. It's really frustrating being super confident walking into a quiz and feeling utterly defeated when you walk away. PS: The class average on one of the quizzes was a 59%. Thankfully they are only 20% of your grade.\nThe final project varies, and you are pretty much required to be in a group. I suggest you look early and in your time zone. My team so far has been great and the content pretty interesting. The Facebook projects are both really complicated and interesting.\nFinally, the workload is probably 15-20 hours a week, much like AI sans the crazy exams. Definitely a more front-loaded course. Overall, good feelings for this course and I learned a ton.",
            rating: 5,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 18, 2020',
            semester: 'fall 2020',
            review: "I recommend this course. While the assignments were rough around the edges as far as deliverables, in 1-2 semesters they should have it down pat. I learned a lot from the coding assignments (4 assignments of about 2 weeks worth of work each) and the project (1 project about a month of work), so in that sense they should be considered well done. The weekly quizzes keep you honest in keeping up with the lectures. Some of them helped me understand the lecture content, while some were more like knowledge checks. I anticipate I will not retain a lot of the info tested on the quizzes, but not so with the assignments.\nThe project is a group project, and (no fault of the class) I had a not-so-good group experience, as happens sometimes in OMSCS. Never-the-less it greatly increased my understanding of the content.\nThe group discussions were ok (4, about a days worth of work each). Reading the paper, writing it up, and commenting on others' write-ups definitely increased my understanding of deep learning, but I would say it failed as a \"discussion\" per se. This is mostly due to the format of Canvas, which alerts you when anyone in the class comments on anyone's response. The only thing to do with the notifications are to ignore them. If we only got notified when our own contribution was commented on, perhaps a better discussion could be facilitated.\nI wish there were more assignments on the later topics like deep reinforcement learning, generative models , and unsupervised learning, which are all very complex and very interesting topics which build on the earlier topics of CNNs, encoders-decoders, and other building blocks.\nThe professor and TAs had great engagement, although that is expected since this is the first semester it was offered.\nOverall, I would say the workload was similar to AI. The course is not as polished, nor the assignments quite as open-ended as the first two in AI, so I would consider AI a better course overall. Hopefully within a couple of semesters this course will get there. I wouldn't recommend waiting for the perfect time, though, as this course is still very good as it stands.\nThe weakest part of the course is the guest lecturers from Facebooks' AI researchers, but it wasn't a huge hindrance, as the lecture slides provided the needed content.",
            rating: 5,
            difficulty: 4,
            workload: 17,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 13, 2020',
            semester: 'fall 2020',
            review: "I really enjoyed this class!\nThere were weekly readings, lectures, and quizzes. I could not figure out how to do well on the quizzes, ended with an 80% average on them. Luckily they were worth only 15% of the final grade, with the lowest one dropped. I found the lectures to be very well done and explained the material well.\nI felt a little personally conflicted that the class was co-taught by Facebook, as I don't think they use deep learning in an ethical manner.\nThe assignments were great! I am a serial procrastinator so by the time I started on it most of the bugs were worked out. The tests and Piazza conversation made them very straightforward and reinforced the material. I didn't personally get much out of the project, as the way we divvied up the work I didn't get my hands as dirty with the DL work.\nOverall I loved this class and think it's a must take in addition to ML and RL for the ML specialization.",
            rating: 5,
            difficulty: 3,
            workload: 10,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 12, 2020',
            semester: 'fall 2020',
            review: "I'm not going to go over what was already discussed, but wanted to chime in with a few of my thoughts:\nPrerequisites:\n\nMachine Learning - the course assumes that you have the necessary knowledge regarding concepts taught in the Machine Learning course, although I wouldn't say it's a hard requirement. Just note that you may lose some points whether it'd be on the quiz or project because you don't know something they assume you already know.\nRead the paper \"The Matrix Calculus You need for Deep Learning\" (https://arxiv.org/abs/1802.01528) and you should be fine with the math in the course:\n\nMy Concerns during the class:\n\n\nLectures were released on a weekly rolling basis. I believe that they heard the negative feedback from students and are now releasing all of the lectures at once.\n\n\nThe lectures provided by Facebook weren't that informative and only provided a really high overview of the topics. They were a very stark contrast to the lectures provided by Dr. Zsolt.\n\n\nQuizzes were either hit or miss: the content that we were tested on was either conceptual or applied and the difficulty ranged from too time consuming or could be completed in under five minutes. For the applied quizzes, we had to do calculations on paper that didn't really feel appropriate. It would have been better off as practice problems. Although towards the latter part of the semester, Dr. Zsolt provided some insight on what was important to know for the quizzes.\n\n\nHowever, I wouldn't worry about the quizzes as they are only worth a total of 15% of your grade. For our term, the lowest score was dropped and we were offered an optional quiz to replace another low score.\n\n\nThe assignments had ambiguity and the instructions were unclear initially, but the TAs worked hard on fixing and making the assignments more clear quickly.\n\n\nThe final project was rough for my group, but in the end we were able to pull it off. I would say to contribute in the Slack channel and figure out who you want to work with early. Ideally, you get a good group and have no hiccups, but anticipate for some problems especially during crunch time.\n\n\nOverall, Deep Learning was my favorite course so far due to the content discussed as well as the teaching staff. It seems like the grading was very lenient for the assignments and with the way the class is weighted, you should be able to get an A (>60% of the class).\nEDIT: The grade distribution ended up being:\nA - 70.0%\nB - 9.3%\nC - 4.0%\nW - 14.0%",
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 11, 2020',
            semester: 'fall 2020',
            review: 'Summary\nOverall, this was a good course, but I think it has the potential to be great with some modifications. The class is heavily front loaded with more difficult assignments and well prepared / delivered lectures by Prof. Kira. In the second half the lecture quality drops substantially with Facebook engineers delivering most of the lessons. This class is technically a “collaboration” with Facebook, which I interpret as Facebook trading lectures for first dibs at recruiting after the course (Georgia Tech definitely got the short end of this stick). The final project is a group project with no individual option, which I felt was more of a lesson in logistics than it was deep learning. Grading is very generous compared to ML, RL, CV.\nProfessor\nProfessor Kira was incredibly engaged. This is my 8th OMSCS class and I think this the most engaged I’ve seen a professor. He clearly has a passion for DL and teaching it. 10/10.\nLectures\nAs I mentioned above, the early lectures are quite good and well organized. Not all information for completing the assignments can be found in the lectures so further reading or researching is required to complete the assignments (which is fine). However, after around the half way point each lecture is taught by a different Facebook engineer. The lectures range in quality, but the average is quite low and I was disappointed that Georgia Tech decided to partner with Facebook on this class.\nAssignments\nThere are a total of four assignments\nAssignment 1 requires implementing the training pipeline (backprop and cost function) for two network architectures. Working through the linear algebra took me some time, but ultimately I thought this was a great project for understanding the math going on under the hood.\nAssignment 2 focuses on CNNs. The first part of the assignment requires implementing a CNN training pipeline from scratch (similar to Assignment 1 except there are some nuances in dealing with the pooling and conv layers). The second part introduces pytorch. Overall, a good assignment, although by this point I felt I’d done enough “implementing from scratch” and was excited to start using pytorch.\nAssignment 3 deals with visualizing network layers and performing style transfer. Overall a really fun project that helped build intuition around how CNNs work.\nAssignment 4 focused on RNNs and encoder / decoder architectures for the application of language translation. I was honestly disappointed by this assignment. The autograders were too simple and didn’t catch bugs early on, which allowed you to get pretty deep into the assignment before you’d find an issue in some block. At the end, it felt mostly like an exercise in debugging tensor dimensions. Even though I received a perfect score on the assignment I felt at the end that I learned little.\nQuizzes\nThere is a proctored quiz each week to check comprehension. The quizzes are overall worth a small part of the grade, but served as good motivation to stay up to date on lectures. I thought this was good.\nGraded Discussions\nThere are four graded discussions where you’re given two papers and asked to choose one to review. Once you’ve submitted your answers you are required to comment on others’ submissions.  Overall, I thought this was a nice way to connect to the literature in the field of deep learning. Reading the papers and throughly understanding them could be time consuming, but I felt I learned quite a bit.\nFinal Project\nStudents are required to form groups of 2 to 4 and either select a project from a list or propose one.  There was no option for individual projects. This was by far my least favorite part of the course. In classes like ML, CV, RL I derived immense value from working through all parts of the problem and coming out the other end feeling satisfaction in the amount I had learned and worked through. Instead, in deep learning we spent way too much time working through logistics of finding common times when we could meet (dealing with jobs, time zones families, etc) and who would do what. I really hope they decide to make the group component of this project optional. I left the group project exhausted and didn’t feel I had learned nearly as much as I should have.\nPros\n\nGood survey of deep learning topics\nGreat professor who is actively engaged in the class\nGaining familiarity with PyTorch\nLearning and working through math of backprop / training\nExposure to some literature in the field\n\nCons\n\nThe group project (did I mention this yet? ;) ).\nLectures delivered by Facebook engineers / the whole Facebook collaboration.  I think Georgia Tech’s decision to partner with Facebook was a mistake. Hopefully they can replace this content in subsequent years.\nSome assignments (namely #4) need some polishing, but I think this will happen over time\n',
            rating: 4,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 10, 2020',
            semester: 'fall 2020',
            review: 'Liked the topics covered. Time consuming. TAs not helpful',
            rating: 2,
            difficulty: 4,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 10, 2020',
            semester: 'fall 2020',
            review: "I was pleased with this class.  It was definitely front-loaded with some very difficult assignments doing hand-crafted backpropagation. Good to understand, but not easy to implement as there are a lot of moving pieces, especially for the CNNs. It felt a little like overkill at times to be working on the backpropagation for CNN when there are so many other things we could be doing, like experimenting with more of the deep architectures, but I guess the class is more about understanding the basics of deep learning, not really playing with the really deep models ourselves.\nThe math for those early quizzes and assignments is quite intense, but by midway through the course there are really no math requirements (as far as work you do, understanding the math behind softmax attention etc. is still important).\nThere were some hiccups with the assignments where they needed to push out several versions as students ran into issues.  I think they got enough feedback that future classes won't have this kind of trouble.  But to this point, I found that the discussions on Slack were invaluable for overcoming errors and learning from others on those harder assignments.  The later assignments were (to me) much easier and I stopped going to Slack for advice.\nThe lectures were pretty good and covered a fair share of breadth and depth.  They had plenty of errata that made reading some slides quite confusing, but I think they've been alerted of most of those issues.  Sometimes the lectures also breeze over the math as if it should be quite clear but to me it wasn't always that way, so be prepared to do some outside studying if you want to really grasp the underlying math.\nOverall though, I really enjoyed it and I learned a lot of things that I think will be very impactful to me and my future career.  I'm also much more confident when reading research papers on the topic.  And I didn't realize how much I would like PyTorch.  Moral of the story is that you should take the class, but definitely hone in your linear algebra and partial derivatives beforehand so you don't have the initial desperate drowning experience at the beginning.\nA note on effort:\nI probably spent about 20 hours a week for the first 6 weeks, but after that, I was able to get things done in about <= 10 hours a week.  I would not recommend pairing with another class unless you have little to no other commitments during the first several weeks.  Depending on your project and team you will need more or less time at the end of the semester.  My team was great and our project was not Facebook level hard, so the last bit was not bad at all.  A good way to finish the program!",
            rating: 4,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 9, 2020',
            semester: 'fall 2020',
            review: "My favourite course alongside RL. I'd say the course is a fair amount of work but a bit too easy. Assignments are front loaded, and the project adds a hefty backload.  Feels like there's a fire hydrant of material to read and study but one can easily do well without it if you so chose. I tried to absorb as much as possible so even it wasnt too difficult it still took a significant amount of time to read and digest. Course staff and Prof are awesome! Interactive and lots of OH.",
            rating: 5,
            difficulty: 3,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 8, 2020',
            semester: 'fall 2020',
            review: "Took a nose dive reluctantly into the first class of DL. I was skeptical and worried about being the first in the line of battle. But boy I am glad I did. Let me first start by introducing the work load.\n11 proctored quizzes spaced every week (there are some off weeks), 15%\n4 Coding assignments, 60%\n4 Graded discussion, 5%\nFinal Project (including proposal), 20%\nAssignment1: Implement ANN from scratch. Here you implement ANN or MLP. You implement the model and the optimizer.\nAssignment2: Build CNN from scratch. You also implement CNN on pytorch and test your model on CIFAR-10 dataset.\nAssignment3: CNN model visualization. You will implement Saliency Maps, Gradcam, Fooling Images and Class Visualization.\nAssignment4: Implement RNNs and LSTM, seq2seq model and Transformers.\nAssignment 1 and 2 were hard. You need to implement ANN and CNN from scratch. You do a forward pass and a back prop. You need to figure out the math for the backward pass by writing the partial derivate on paper and use chain rule as the gradient flows backward. While the code itself is not hard and only few lines per layer, figuring out the math and getting an intuition is hard.\nAssignment 3 was all about visualization of CNN. You have to implement algorithms that will visualize parts of an image that the CNN uses (activations) in its decision making.\nAssignment 4 was all about RNNs. RNNs are hard and the most difficult of all Neural network algorithms to understand and get an intuition on. You won't implement RNNs from scratch thankfully but use pytorch to implement LSTMs, seq2seq and Transformer models.\nThe weekly quizzes forces you to periodically watch the lectures as they are released at least twice. While the quiz itself was not hard, preparing for the quiz was stressful.\nThere are 4 graded discussion where you have to read a paper they ask. You pick one of the 2 papers and post a short review on it and also answer 2 questions on it. You then post your response on Canvas. You then have to read at least 2 response from other students and comment on their post. This is not a peer review but the GD is designed to exchange thoughts and ideas with other students. While not very hard, it is time consuming. You need to spend at least 4-5 hours on this task.\nFinally there is a group project. For this term you were not allowed to work solo and groups were mandatory. You can choose your project or choose from one of the Facebook project. The Facebook projects are all real life and hard. The data itself will run into 100s of GBs and a GPU is a must for most of them. So choose them with caution. Using online cloud resource is not very practical. I would recommend that at least one member of the team has a 1080 ti or better GPU available to run the code locally.\nOverall this class gets the job done in explaining practical ML. While I won't call this class very hard, it is stressful for sure. Prof Kira and his TAs did a fantastic job. The first class had only 150 students and were all OMSCS veterans and very motivated. The drop rate was only 10% for this term. The TAs were on campus students who took this class while on campus. Overall the quality of students in this class was very high due to limited class strength. It felt like an on campus class unlike other classes I have taken in OMSCS. But I don't expect this experience to continue with increased class size. The first class was super competitive and the class mean was very high. Thankfully the class is not curved and 90 or above constitute an A. It is not difficult to get an A if you do well on the project and the assignments. This class is a must take for ML specialization students.",
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 8, 2020',
            semester: 'fall 2020',
            review: "This is the type of course I was hoping all courses in OMSCS would be like when I enrolled. Great combination of theory and practice. Paper discussions keep you updated on some of the many new developments that are always occurring. The assignments are solid and the open-ended nature of the final project is awesome. Professor Kira is great and is really involved in the class. One major caveat with my review is that there were only 150 seats in this first iteration, so hopefully the quality can be maintained (looks like next term it's 500 + 50 for OMSA). On the other hand, there were some kinks with the assignments and quizzes to work out this first term which shouldn't be as prevalent in the future. Since DL is an area so many students are really excited about, there's a lot of good discussion and helpful students on piazza and in slack. Awesome course and really glad I was able to squeeze it in in the last term!",
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 8, 2020',
            semester: 'fall 2020',
            review: "This class is a must take if you're interested in ML or doing the ML specialization. The assignments in DL have you implement a bunch of fundamental things from scratch including neural networks (forward and backward pass), convolutional neural networks (forward and backward pass). But it's not all from scratch; after implementing a lot of fundamentals from scratch, the assignments have you use PyTorch to explore more advanced topics. Implementing things from scratch also help you better understand how all of the components of PyTorch fit together like lego blocks and how you can modify different parts to achieve different goals. One interesting application was in an assignment where we had to create custom loss functions and then use PyTorch's built in backprop to optimize over images to do style-transfer. I hadn't been exposed to this side of DL--using backprop to optimize over your input, so it was quite illuminating to me. We also explored how we can optimize over image to visualize what CNNs are learning, and generate adversarial examples that can expose the blind spots of a neural network. In the last assignment, we used PyTorch to implement recurrent neural networks, LSTMs, and transformers to perform neural machine translation. We couldn’t use the high level APIs, but instead had to implement custom nn.Modules that defined the forward passes based on the equations. Luckily, in this assignment we could leverage the back-prop utility baked into PyTorch, so we didn’t need to implement the backward pass.\nIt's also worth noting that parts of the class were developed by FB AI researchers. Some of the project prompts were defined by them (Motion prediction, Self-supervised MRI reconstruction, Hateful Memes Classification, Natural Language Translation Quality Estimation, something about Transformers/Adapters). During the last month, we actually had 9 Office Hours scheduled with the FB AI researchers in a wide array of topics (Data Wrangling, Transformers, Neural Machine Translation, Language Models, Scaling Deep Learning from Experiment to Production, PyTorch & Scalable Training, Fairness, Accountability and Transparency (FAT), World2Vec).\nBut wait, there's more! There are weekly proctored quizzes. They were pretty annoying, but I think they're annoying in a good way. Assuming you want to do well on the quiz, they force you to review the lectures perhaps a 2nd time and practice a few calculations on paper to make sure you really did understand what you saw. In many other classes, you're not really assessed on your understanding of every detail in the lectures, so it's fine to just breeze through them not really caring to make sure it all sticks.\nOverall, DL really increases your depth of understanding of various topics in ML. I've taken ML, and RL, but DL is the class that puts the ML in the ML specialization.",
            rating: 5,
            difficulty: 4,
            workload: 20,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 8, 2020',
            semester: 'fall 2020',
            review: "IMO this course is the only one that is comparable to the graduate-level, on-campus version of course offered in Top-level colleges such as Standford CNN course (cs231n). I felt I'm fortunate to take this course as the last course before graduation.\nAs a person who watched all the lectures and did assignments of cs231n before, I felt this course is similar to cs231n in terms of high-quality of lectures, the difficulty of assignments, and open-ended projects including FB's ideas which are not trivial but promising. I was also surprised about active engagements from both Prof.Kira and TAs.\nHere are a few tips:\nAssignments: you need to clearly understand basic DL algorithms such as forward/backward pass so that you can implement this in the lowest level using numpy. For assignments 3 and 4, you need to implement recent DL models using pytorch. I'd recommend to brush up your numpy skill and watch tutorials about pytorch unless you have experience with DL frameworks.\nQuiz: quiz is provided weekly and most of the problems are either conceptual or simple calculations. The only thing to do well in the quiz is understanding lectures well, reviewing the contents thoroughly, and don't abstract them. Hints from Prof.Kira were also helpful.\nProject: you can propose any subject regarding DL or tackle one of FB's ideas. You might need GPUs or additional cloud credits if you tackle FB projects as those might involve heavy computation. Team project would be recommended but you can do it alone if you have a thorough idea about your topic. Personally, I took one of FB projects as I have quite decent GPUs and those seem promising and potentially publishable ideas. If you want to shoot for publication, I'd recommend tackling FB ideas as most of the projects are well-organized and straight-forward to do though those aren't trivial.",
            rating: 5,
            difficulty: 3,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 7, 2020',
            semester: 'fall 2020',
            review: "Overall a great and long overdue DL course. My one complaint is that it took GT so long. Should be a course you take right at the beginning after taking AI so you start your OMSCS journey in ML with a good foundation of what you want to focus on more.\nI'm glad we finally got a rigorous academic course on DL that really tries to delve in to the mathematical nuts and bolts of the algorithms even if it ignores most theoretical aspects which imo is fine given the amount of material to cover. I have said this many times before but DL is not something you just learn through cheap MOOCs and this class proves why.\nThis class is not difficult in the unreasonable brutish kind of way. It is however a time consuming course in the sense that you need to code everything and assignment points are all or nothing pretty much and you never have a week off because there is something due every week.\nOverall though it ranked in the middle for me time wise averaging 15 hours/week and I took this with GA which I averaged 20 hrs/week.\n4 assignments with only one  (A2) I would classify as Hard. A1 took me 15 hrs and is a fairly gentle introduction to neural networks, A2 30 hours was the hardest and involves some beastly level of coding and derivations, A3 the easiest to implement 20 hrs, and A4 took 20 hrs but has a lot of material to cover.\nThe quizzes are mostly jokes with a few math ones that have no business being quizzes. But even if you fail those it makes little impact on your grade.\nThe project can be as easy or hard as you want it to be. Group format makes it a pain. I am yet to see the grade but I would advise you to not do the FB projects if you are very new to DL as those are open research topics and absolutely not meant to be trivial\nPros:\n\n\nProf Kira has some of the best lectures and some of the best engagement I have seen in any class. Some of his lectures on CNNs I truly have not found anywhere which cover the material with the same kind of rigor. Thank you Professor for finishing my experience on a high not. Coming into OMSCS i thought this is what the program would be like. I am glad i got at least this class (+AI4R and AI) where it felt like I was in a class and not some cheap autopilot MOOC.\n\n\nStudent quality was some of the best and the small size makes for excellent discussions. Unfortunately this may decline next semester with a bigger cohort that is not biased towards curious and motivated OMSCS veterans.\n\n\nProject is a good way to get exposure to a problem you want to learn about. I spent about 100 hours only because I enjoyed my problem. You definitely don't have to. 40-50 hours sounds more reasonable\n\n\nCons:\n\n\nFacebook lectures and involvement in general is actually very bad. Some of the FB lecturers (SWES and PMS) are probably less knowledgeable about DL than the top 10% of the student body taking the class. Their so called projects are open source data competitions which you don't need to take this class for. It is disappointing that FB wanted to be involved in this course and yet they won't provide any of their data or cloud GPU resources for us to work with. They scheduled mostly useless office hours (during work hours too) and their project involvement included maybe 1 or 2 hours talking to someone who didn't really care much about non PhD students. An unethical place like FB should also not be teaching an AI Ethics module. I found it very tone deaf and marginalizing for GT to let FB speak on this sensitive topic.\n\n\nWeekly quizzes are too many and can be useless T/F type questions. They need to be condensed to 5 or 6 biweekly and the math questions are not suited for a quiz format. Either put them as part of take home problem sets or provide examples before the quiz.\n\n\nGroup project format does not work for DL research work environments in real life. DL is not SWE work. It is not PM work. DL requires independent research and people collaborate only when their research interests align and they have something of value to add. Best solution would be to what CV or BD4H does. Give a list of predetermined and vetted 6 projects and you get 3 weeks to implement 1 or 2 research papers from scratch on your own. There are also some teams I think that imploded because there are too many hyper competitive types in this class who want to prove how smart they are to everyone at the expense of actually writing an Introduction to DL level paper. At work people would be fired immediately if there was a mismatch of expectations and skills at most places something you can't do in a class format.\n\n",
            rating: 4,
            difficulty: 3,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 6, 2020',
            semester: 'fall 2020',
            review: "Overall, I thought this was an excellent course. One of my favorites in the entire program. This was my 8th class. The class provided a great split of foundational knowledge/depth and higher level breadth and exposure to deep learning topics.\nThere were 4 programming assignments. The first 2 have you implementing neural networks from scratch. These were the most difficult assignments as they require you to have a good handle on linear algebra and the chain rule in calculus. The math wasn't super hard, but keeping track of everything when you have hundreds of variables across multiple network layers got complicated. I hadn't done calculus or linear algebra in ~5 years so I was scrambling to re-learn for the first month of class.\nThe last 2 are higher level assignments where you use PyTorch to implement different network architectures. I had no pytorch experience before this class and thought that the assignments did a good job of teaching you the library. You won't suffer too much if you haven't used it before. As others have mentioned, the quizzes were fairly difficult and you can spend a lot of time preparing for them if you want good grades on them. Luckily they weren't worth too much of your final grade, so it doesn't hurt you too much if you bomb a few of them.\nThe assignments all had several included unit tests that the TAs wrote along with an autograder. These helped immensely since it can be hard to tell if your code is doing the math correctly. As others have said, there were some assignments that the TAs were fixing problems with while students were actively working on the project, but that's to be expected in a new course. The TAs and the professor were always very responsive on Piazza. This is one of the few classes I've taken where the professor is actively engaging with the students, even on the Piazza posts that were just discussions rather than post about the lectures/assignments.\nThe class also included several \"graded discussions\" where you chose 1 paper out of a few that the professor selected, read the paper, answered some questions about it and provided your opinions, then responded to a few other students posts. These were always very enjoyable. Many times, we were reading papers that had been published within the last year. It was really cool to be able to see what researchers were actively working on and be able to understand what they're talking about in the papers. In other classes were we read academic papers, they were always super old papers, or the material felt very inaccessible to someone who wasn't already a PhD in the field. These deep learning papers always felt somewhat accessible, even if I wouldn't be capable of re-implementing them.\nThe final project had you in groups of 3/4 students working on a project of your choice. As I write this review, the project is due tonight, so I don't yet have a grade for it. But the TAs and professor have said the grade is based more on what you learned and could articulate in your final report than how good your results are. As with all group projects, if you are proactive and find your own group members before the project starts, you will probably have a good time. If you get auto-assigned to a group, you will probably have a bad time.\nOverall, this is one of the best courses in the program that I would recommend anyone take, even if you're not doing the ML specialty (I am not doing ML). The professor, TAs, and content are all top notch.",
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 4, 2020',
            semester: 'fall 2020',
            review: 'Deep learning CS7643 single-handedly changed how I felt about machine learning at OMSCS.\nI have taken numerous other machine learning courses and always felt machine learning CS7641 never did the subject justice.  Taken in isolation, 7641 machine learning is at best an okay course and when compared to how other programs and departments teach machine learning, CS7641 really pales in comparison.  By contrast, deep learning CS7643 is really one of the best courses whether you evaluate it in isolation or you compare it to similar courses at other universities.\nSimply put I think CS7643 is THE course to take in OMSCS whether you are in the ML track or not.\nThis course isn\'t just a "run the model with an ML package" type of course.  In the first half you will implement large parts of multi-layer perceptrons and CNNs by hand and including back propagation for these models.  This really makes you learn in detail how these models work under the hood and is already far more detailed than how many other courses treat this part of the material.\nThe second half the course includes older topics LSTMs and sequence to sequence models but also more recent topics like attention and transformers.\nThere is a proctored quiz roughly weekly but it\'s not a big deal if you read over the slides especially the parts the instructors say are important.',
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 4, 2020',
            semester: 'fall 2020',
            review: "Overall I did like this course, and the material covered is very good and in-depth. Without this course and the materials covered, I don't feel like the Machine\nLearning specialization would have been complete, by far.\nThe material covered is essential for understanding deep learning techniques.\nBy that criteria, this course is a must-take.\nAdditional considerations:\n\nThe professor and TAs are engaged and quick to interact via Piazza and even in the Slack channel. Definitely appreciate the engagement and help there.\nThe assigned papers were cutting edge, and essential reading. The assigned book is OK, and a bit dry, but it's fair game for the quizzes.\nThe weekly quizzes got old fast, and they are not easy. Some nights you have to forgo working on a given assignment to get through preparing for the quizzes, and that caused some frustration. If you have limited time on nights and weekends, this needs to be managed.\nThe assignments went through some churn fixing errors as can be expected for a first-run course, but eventually got sorted. If they go with the final versions of the project descriptions and codebase for future semesters, this shouldn't be an issue.\nI miss (maybe just nostalgia at this point) the Udacity videos with their built-in quizzes. That format with bite-sized sections in the 5-8 minute mark would be better to break up the material and have details sink in. Sometimes felt like a grind to get though 25 minute videos  (especially the Facebook ones) where the audio is not always crisp and there is a lot of information covered quickly.\nWorking on the assignments brought back some flashbacks from AI, where you're searching for some clue to break through an issue and the clue is buried deep in the assignment Piazza post, or even in the Slack (which had a lot of activity, not always pertinent).\nGot a great working knowledge of Pytorch and its peculiarities, something you only get by banging your head against the wall solving tensor size mismatch issues.\n\nOverall, if you are here for ML, you can't skip this course. Without this knowledge, you'd be shooting in the dark when working with deep neural network architectures.",
            rating: 4,
            difficulty: 4,
            workload: 18,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 4, 2020',
            semester: 'fall 2020',
            review: 'One of the best course in omscs. Workload is high but the content is amazing and so latest. Learning pytorch and implementing latest research papers were so much fun. This was the only course till now which I never wished to end.',
            rating: 5,
            difficulty: 4,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 2, 2020',
            semester: 'fall 2020',
            review: "The toughest and most rewarding class in the program. You will have a very thorough understanding of how neural networks are designed from scratch after taking this class through rigorous programming exercises. The workload for the class surpassed ML. Do NOT pair this with another class unless you're full-time. Huge shout out to the professor and TAs for being extremely active on Piazza and willing to make adjustments in this first semester as was deemed appropriate (shifting deadlines, updating assignments, correcting quiz errors).",
            rating: 5,
            difficulty: 5,
            workload: 25,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 1, 2020',
            semester: 'fall 2020',
            review: 'Excellent course. Good assignments and lectures. A must take for students interested in ML.',
            rating: 5,
            difficulty: 5,
            workload: 15,
        },
        {
            user: 'Georgia Tech Student',
            reviewDate: 'December 1, 2020',
            semester: 'fall 2020',
            review: 'Really a great class. Lots of math at the beginning, need to calculate chain rule by hand for backprop, convolutions by hand ..etc. Was able to touch on most of the major DL topics, GANs, VAEs, CV, NLP. Would 100% recommend.\nWould be hard to pair with another course.',
            rating: 5,
            difficulty: 5,
            workload: 15,
        },
    ],
};
